{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# VITRA Inference Demo\n",
                "\n",
                "This notebook allows for interactive inference with the VITRA model. You can modify inputs and re-run inference without reloading the model."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Environment Setup & Imports\n",
                "import os\n",
                "import sys\n",
                "\n",
                "# Change working directory to project root\n",
                "# This is crucial for relative paths (like weights loading) to work correctly\n",
                "try:\n",
                "    # Assuming notebook is in my_demo/ folder, root is one level up\n",
                "    project_root = os.path.abspath(\"..\")\n",
                "    os.chdir(project_root)\n",
                "    print(f\"Changed working directory to: {os.getcwd()}\")\n",
                "    \n",
                "    # Add project root to path\n",
                "    if project_root not in sys.path:\n",
                "        sys.path.append(project_root)\n",
                "except Exception as e:\n",
                "    print(f\"Error setting working directory: {e}\")\n",
                "\n",
                "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
                "\n",
                "import json\n",
                "import torch\n",
                "import numpy as np\n",
                "from PIL import Image\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "from vitra.models import VITRA_Paligemma, load_model\n",
                "from vitra.utils.data_utils import resize_short_side_to_target, load_normalizer\n",
                "from vitra.datasets.human_dataset import pad_state_human, pad_action\n",
                "from vitra.utils.config_utils import load_config\n",
                "from vitra.datasets.dataset_utils import (\n",
                "    ActionFeature,\n",
                "    StateFeature,\n",
                ")\n",
                "\n",
                "print(\"Libraries imported successfully.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. Load Model & Config (Run Once)\n",
                "# This step loads the heavy model weights to GPU.\n",
                "\n",
                "# Load configs\n",
                "configs = load_config('VITRA-VLA/VITRA-VLA-3B')\n",
                "\n",
                "# Override config if provided\n",
                "pretrained_path = 'VITRA-VLA/VITRA-VLA-3B'\n",
                "statistics_path = 'VITRA-VLA/VITRA-VLA-3B'\n",
                "configs['model_load_path'] = pretrained_path\n",
                "configs['statistics_path'] = statistics_path\n",
                "\n",
                "print(\"Loading model... (this may take a minute)\")\n",
                "# Load model and normalizer\n",
                "model = load_model(configs).cuda()\n",
                "model.eval()\n",
                "\n",
                "normalizer = load_normalizer(configs)\n",
                "print(\"Model loaded successfully!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3. Load & Process Image\n",
                "image_path = \"image.png\"  # Relative path in my_demo folder\n",
                "\n",
                "try:\n",
                "    # Handle path relative to notebook location\n",
                "    if not os.path.exists(image_path):\n",
                "        # Try looking in parent directory or absolute path\n",
                "        # Adjust this if your image is elsewhere\n",
                "        print(f\"Image not found at {image_path}, checking current dir...\")\n",
                "        image_path = \"image.png\"\n",
                "        \n",
                "    image_pil = Image.open(image_path)\n",
                "    image_resized = resize_short_side_to_target(image_pil, target=224)\n",
                "    \n",
                "    # Convert to numpy for model input\n",
                "    image_np = np.array(image_resized)\n",
                "    print(f\"[DEBUG] Image shape: {image_np.shape}\")\n",
                "    \n",
                "    # Display image\n",
                "    plt.imshow(image_np)\n",
                "    plt.axis('off')\n",
                "    plt.show()\n",
                "\n",
                "except Exception as e:\n",
                "    print(f\"Error loading image: {e}\")\n",
                "    # Create dummy image for testing if file missing\n",
                "    image_np = np.zeros((224, 224, 3), dtype=np.uint8)\n",
                "    print(\"Using dummy black image.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4. Prepare Input Data (Edit this cell to change inputs)\n",
                "\n",
                "# --- Parameters ---\n",
                "instruction = \"Left hand: None. Right hand: Pick up the phone on the table.\"\n",
                "fov_deg = 60.0\n",
                "\n",
                "# State Construction\n",
                "# Total dimension: 122\n",
                "state = np.zeros((normalizer.state_mean.shape[0],)) \n",
                "print(f\"[DEBUG] Initial State shape: {state.shape}\")\n",
                "\n",
                "# Masks\n",
                "# state_mask: [Left, Right]\n",
                "state_mask = np.array([False, True], dtype=bool)\n",
                "print(f\"[DEBUG] State Mask shape: {state_mask.shape}\")\n",
                "\n",
                "# action_mask: [Time, 2]\n",
                "# action_mask[:, 0] = Left, action_mask[:, 1] = Right\n",
                "action_mask = np.tile(np.array([[False, True]], dtype=bool), (model.chunk_size, 1))  \n",
                "print(f\"[DEBUG] Action Mask shape: {action_mask.shape}\")\n",
                "\n",
                "# --- Preprocessing ---\n",
                "# FOV to tensor\n",
                "fov = torch.tensor([[np.deg2rad(fov_deg), np.deg2rad(fov_deg)]], dtype=torch.float32).cuda()\n",
                "\n",
                "# Normalize state\n",
                "norm_state = normalizer.normalize_state(state)\n",
                "print(f\"[DEBUG] Normalized State shape: {norm_state.shape}\")\n",
                "\n",
                "# Unified dimensions\n",
                "unified_action_dim = ActionFeature.ALL_FEATURES[1]   # 192\n",
                "unified_state_dim = StateFeature.ALL_FEATURES[1]     # 212\n",
                "\n",
                "# Padding\n",
                "unified_state, unified_state_mask = pad_state_human(\n",
                "    state = norm_state,\n",
                "    state_mask = state_mask,\n",
                "    action_dim = normalizer.action_mean.shape[0],\n",
                "    state_dim = normalizer.state_mean.shape[0],\n",
                "    unified_state_dim = unified_state_dim,\n",
                ")\n",
                "_, unified_action_mask = pad_action(\n",
                "    actions=None,\n",
                "    action_mask=action_mask,\n",
                "    action_dim=normalizer.action_mean.shape[0],\n",
                "    unified_action_dim=unified_action_dim\n",
                ")\n",
                "\n",
                "print(f\"[DEBUG] Unified State shape: {unified_state.shape}\")\n",
                "print(f\"[DEBUG] Unified State Mask shape: {unified_state_mask.shape}\")\n",
                "print(f\"[DEBUG] Unified Action Mask shape: {unified_action_mask.shape}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 5. Run Inference\n",
                "print(f\"Running inference for instruction: '{instruction}'\")\n",
                "\n",
                "with torch.no_grad():\n",
                "    norm_action = model.predict_action(\n",
                "        image = image_np,\n",
                "        instruction = instruction,\n",
                "        current_state = unified_state.unsqueeze(0), # Add batch dim\n",
                "        current_state_mask = unified_state_mask.unsqueeze(0),\n",
                "        action_mask_torch = unified_action_mask.unsqueeze(0),\n",
                "        num_ddim_steps = 10,\n",
                "        cfg_scale = 5.0,\n",
                "        fov = fov,\n",
                "        sample_times = 1\n",
                "    )\n",
                "\n",
                "print(f\"[DEBUG] Raw Model Output shape: {norm_action.shape}\")\n",
                "\n",
                "# Extract valid action part (first 102 dims)\n",
                "# Output shape is [Batch, Time, Dim]\n",
                "valid_norm_action = norm_action[0, :, :102]\n",
                "\n",
                "# Denormalize\n",
                "unnorm_action = normalizer.unnormalize_action(valid_norm_action)\n",
                "\n",
                "print(f\"[DEBUG] Final Unnormalized Action shape: {unnorm_action.shape}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 6. Results Analysis\n",
                "print(\"Predicted Action (First 5 steps):\")\n",
                "print(unnorm_action[:5])\n",
                "\n",
                "# Optional: Plot trajectory of wrist position (indices 0-3 for left, 51-54 for right)\n",
                "traj_left = unnorm_action[:, 0:3]\n",
                "traj_right = unnorm_action[:, 51:54]\n",
                "\n",
                "fig = plt.figure(figsize=(12, 5))\n",
                "\n",
                "# Right Hand Trajectory\n",
                "ax1 = fig.add_subplot(121, projection='3d')\n",
                "ax1.plot(traj_right[:, 0], traj_right[:, 1], traj_right[:, 2], 'r-', label='Right Hand')\n",
                "ax1.set_title(\"Right Hand Wrist Trajectory\")\n",
                "ax1.set_xlabel('X')\n",
                "ax1.set_ylabel('Y')\n",
                "ax1.set_zlabel('Z')\n",
                "\n",
                "# Left Hand Trajectory\n",
                "ax2 = fig.add_subplot(122, projection='3d')\n",
                "ax2.plot(traj_left[:, 0], traj_left[:, 1], traj_left[:, 2], 'b-', label='Left Hand')\n",
                "ax2.set_title(\"Left Hand Wrist Trajectory\")\n",
                "ax2.set_xlabel('X')\n",
                "ax2.set_ylabel('Y')\n",
                "ax2.set_zlabel('Z')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
