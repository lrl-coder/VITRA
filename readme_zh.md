<div align="center">

<h1 align="center"><span
    style="font-family: 'Courier New', Courier, monospace; font-size: 115%;"><span style="font-size: 130%;">V</span>ITRA</span>:<br><span
    style="font-size:2.22rem;">åˆ©ç”¨ç°å®äººç±»æ´»åŠ¨è§†é¢‘è¿›è¡Œæœºå™¨äººæ“çºµçš„<br>å¯æ‰©å±•è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹é¢„è®­ç»ƒ
    </span></h1>

<p align="center">
    <a href="https://arxiv.org/abs/2510.21571"><img src='https://img.shields.io/badge/arXiv-Paper-red?logo=arxiv&logoColor=white' alt='arXiv'></a>
    <a href='https://microsoft.github.io/VITRA/'><img src='https://img.shields.io/badge/Project_Page-Website-green?logo=googlechrome&logoColor=white' alt='Project Page'></a>
    <a href='https://huggingface.co/VITRA-VLA/VITRA-VLA-3B'><img src='https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Model-blue'></a>
    <a href='https://huggingface.co/datasets/VITRA-VLA/VITRA-1M'><img src='https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Data-yellow'></a>
    <a href='LICENSE'><img src='https://img.shields.io/badge/License-MIT-orange' alt='License'></a>
</p>

<p align="center"><img src="assets/teaser.jpg" width="100%" alt="VITRA Teaser"></p>

<div align="justify">
<span style="font-family: 'Courier New', Courier, monospace; font-size: 115%;"><span style="font-size: 130%;">V</span>ITRA</span> æ˜¯ä¸€ç§åˆ©ç”¨å¤§è§„æ¨¡ã€æ— è„šæœ¬ã€çœŸå®ä¸–ç•Œäººç±»æ‰‹éƒ¨æ´»åŠ¨è§†é¢‘æ¥é¢„è®­ç»ƒæœºå™¨äººæ“çºµè§†è§‰-è¯­è¨€-åŠ¨ä½œ (VLA) æ¨¡å‹çš„æ–°æ–¹æ³•ã€‚æˆ‘ä»¬å°†äººæ‰‹è§†ä¸ºçµå·§çš„æœºå™¨äººæœ«ç«¯æ‰§è¡Œå™¨ï¼Œè¯æ˜äº†æ²¡æœ‰ä»»ä½•æ ‡æ³¨çš„çœŸå®é‡å¤–ç¬¬ä¸€è§†è§’ï¼ˆegocentricï¼‰äººç±»è§†é¢‘å¯ä»¥è½¬æ¢ä¸ºåœ¨ä»»åŠ¡ç²’åº¦å’Œæ ‡ç­¾æ–¹é¢ä¸ç°æœ‰æœºå™¨äºº V-L-A è®­ç»ƒæ•°æ®å®Œå…¨å¯¹é½çš„æ•°æ®æ ¼å¼ã€‚æˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªåŒ…å«è¶…è¿‡ 100 ä¸‡ä¸ªç‰‡æ®µçš„äººæ‰‹ V-L-A æ•°æ®é›†ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥å¼€å‘äº†ä¸€ä¸ªåœ¨è¯¥æ•°æ®é›†ä¸Šè®­ç»ƒçš„ã€å¸¦æœ‰å› æœåŠ¨ä½œ Transformer çš„ VLA æ¨¡å‹ã€‚å®ƒåœ¨å…¨æ–°åœºæ™¯ä¸­å±•ç°äº†å¼ºå¤§çš„é›¶æ ·æœ¬ï¼ˆzero-shotï¼‰äººæ‰‹åŠ¨ä½œé¢„æµ‹èƒ½åŠ›ï¼Œå¹¶ä½œä¸ºå®ç‰©æœºå™¨äººæ“çºµçš„å°‘æ ·æœ¬å¾®è°ƒå’Œé€‚é…çš„åŸºçŸ³ã€‚
<br>
<br>

***æœ‰å…³è§†é¢‘æ¼”ç¤ºï¼Œè¯·å‚è€ƒæˆ‘ä»¬çš„ [é¡¹ç›®ä¸»é¡µ](https://microsoft.github.io/VITRA/)ã€‚***
</div>

<br>

</div>

---

## ğŸš© æ–°é—»ä¸æ›´æ–°
*   **[2025-12-05]** ğŸš€ å‘å¸ƒä½¿ç”¨å•å¼ å›¾åƒè¿›è¡Œé›¶æ ·æœ¬æ¨ç†çš„ä»£ç ã€‚
*   **[2025-11-30]** ğŸš€ æˆ‘ä»¬çš„ä»£ç ã€é¢„è®­ç»ƒæ¨¡å‹å’Œæ•°æ®é›†ç°å·²å¼€æºã€‚
*   **[2025-10-24]** ğŸš€ **VITRA** è®ºæ–‡åœ¨ arXiv ä¸Šå‘å¸ƒã€‚

---
## ğŸ¤— é¢„è®­ç»ƒæ¨¡å‹ä¸æ•°æ®é›†

æˆ‘ä»¬çš„é¢„è®­ç»ƒæ¨¡å‹å’Œæ•°æ®é›†å¯åœ¨ Hugging Face Hub ä¸Šè·å–ï¼š
<table>
  <thead>
    <tr>
      <th>Hugging Face æ¨¡å‹</th>
      <th>å‚æ•°é‡</th>
      <th>æè¿°</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><a href="https://huggingface.co/VITRA-VLA/VITRA-VLA-3B" target="_blank"><code>VITRA-VLA-3B</code><a></td>
      <td style="font-size: 0.92em;">3B</td>
      <td style="font-size: 0.92em;">åŸºäºäººæ‰‹æ•°æ®é¢„è®­ç»ƒçš„åŸºç¡€ VLA æ¨¡å‹ã€‚</td>
    </tr>
  </tbody>
</table>

**æ³¨æ„ï¼šæˆ‘ä»¬çš„åŸºç¡€ VLA æ¨¡å‹æ˜¯ä» [Paligemma2](https://huggingface.co/google/paligemma2-3b-mix-224) å¾®è°ƒè€Œæ¥çš„ã€‚å¦‚æœæ‚¨æ— æ³•è®¿é—® [Paligemma2](https://huggingface.co/google/paligemma2-3b-mix-224)ï¼Œè¯·åœ¨ [å®˜æ–¹ç½‘ç«™](https://huggingface.co/google/paligemma2-3b-mix-224) ä¸Šç”³è¯·æƒé™ã€‚**

<table>
  <thead>
    <tr>
      <th rowspan="2">Hugging Face æ•°æ®é›†</th>
      <th colspan="2" style="text-align: center;">å­æ•°æ®é›†</th>
    </tr>
    <tr>
      <th>æ•°æ®é›†åç§°</th>
      <th>ç‰‡æ®µæ•°é‡ (Episodes)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td rowspan="6"><a href="https://huggingface.co/datasets/VITRA-VLA/VITRA-1M" target="_blank"><code>VITRA-1M</code></a></td>
      <td><code>ego4d_cooking_and_cleaning</code></td>
      <td>454,244</td>
    </tr>
    <tr>
      <td><code>ego4d_other</code></td>
      <td>494,439</td>
    </tr>
    <tr>
      <td><code>epic</code></td>
      <td>154,464</td>
    </tr>
    <tr>
      <td><code>egoexo4d</code></td>
      <td>67,053</td>
    </tr>
    <tr>
      <td><code>ssv2</code></td>
      <td>52,718</td>
    </tr>
    <tr>
      <td><strong>æ€»è®¡</strong></td>
      <td><strong>1,222,918</strong></td>
    </tr>
  </tbody>
</table>

**æ³¨æ„ï¼šè¯¦è§ [`data/data.md`](data/data.md) ä»¥è·å–æœ‰å…³æˆ‘ä»¬æ•°æ®é›†çš„è¯¦ç»†ä¿¡æ¯ã€‚**

## ğŸ“‘ ç›®å½•
- [1. å®‰è£…](#1-å®‰è£…)
  - [1.1 è®­ç»ƒ / æ¨ç†è¦æ±‚](#11-è®­ç»ƒ--æ¨ç†è¦æ±‚)
  - [1.2 å¯è§†åŒ–è¦æ±‚](#12-å¯è§†åŒ–è¦æ±‚)
- [2. ä½¿ç”¨äººæ‰‹å›¾åƒè¿›è¡Œæ¨ç†](#2-ä½¿ç”¨äººæ‰‹å›¾åƒè¿›è¡Œæ¨ç†)
- [3. ä½¿ç”¨è‡ªå®šä¹‰æœºå™¨äººæ•°æ®é›†è¿›è¡Œå¾®è°ƒ](#3-ä½¿ç”¨è‡ªå®šä¹‰æœºå™¨äººæ•°æ®é›†è¿›è¡Œå¾®è°ƒ)
  - [3.1 æ•°æ®å‡†å¤‡](#31-æ•°æ®å‡†å¤‡)
  - [3.2 å®ç°è‡ªå®šä¹‰ RoboDatasetCore](#32-å®ç°è‡ªå®šä¹‰-robodatasetcore)
  - [3.3 è®¡ç®—æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯](#33-è®¡ç®—æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯)
  - [3.4 ä¿®æ”¹é…ç½®](#34-ä¿®æ”¹é…ç½®)
  - [3.5 è¿è¡Œè„šæœ¬](#35-è¿è¡Œè„šæœ¬)
- [4. å®ç‰©éƒ¨ç½²](#4-å®ç‰©éƒ¨ç½²)
- [5. äººæ‰‹ VLA æ•°æ®é›†åˆ©ç”¨](#5-äººæ‰‹-vla-æ•°æ®é›†åˆ©ç”¨)
- [6. ä»å¤´å¼€å§‹è¿›è¡Œäººç±»æ•°æ®é¢„è®­ç»ƒ](#6-ä»å¤´å¼€å§‹è¿›è¡Œäººç±»æ•°æ®é¢„è®­ç»ƒ)
  - [6.1 æ•°æ®é›†å‡†å¤‡](#61-æ•°æ®é›†å‡†å¤‡)
  - [6.2 è®¡ç®—æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯](#62-è®¡ç®—æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯)
  - [6.3 ä¿®æ”¹é…ç½®](#63-ä¿®æ”¹é…ç½®)
  - [6.4 è¿è¡Œè„šæœ¬](#64-è¿è¡Œè„šæœ¬)
- [å¼•ç”¨](#å¼•ç”¨)


---

## 1. å®‰è£…
### 1.1 è®­ç»ƒ / æ¨ç†è¦æ±‚
æˆ‘ä»¬å»ºè®®ä½¿ç”¨ `conda` ç®¡ç†ç¯å¢ƒã€‚éœ€è¦ PyTorch >= 2.3.0 å’Œ CUDA >= 12.1ï¼ˆè¾ƒä½ç‰ˆæœ¬ä¹Ÿå¯èƒ½è¿è¡Œï¼Œä½†æˆ‘ä»¬å°šæœªæµ‹è¯•ï¼‰ã€‚å¦‚æœç¯å¢ƒä»…ç”¨äºè®­ç»ƒï¼Œå»ºè®®ä½¿ç”¨æ›´é«˜ç‰ˆæœ¬çš„ PyTorch ä»¥è·å¾—æ›´å¿«çš„è®­ç»ƒé€Ÿåº¦ã€‚

```bash
# å…‹éš†ä»“åº“
git clone https://github.com/microsoft/VITRA.git
cd VITRA

# åˆ›å»ºç¯å¢ƒ
conda create -n vitra python=3.10 -y
conda activate vitra

# å®‰è£…ä¾èµ–
pip install -e .
```

<details>
<summary>ç‚¹å‡»æŸ¥çœ‹è¯¦ç»†ç³»ç»Ÿè¦æ±‚</summary>

*   **æ“ä½œç³»ç»Ÿ**: Linux (æ¨è Ubuntu 20.04/22.04)
*   **Python**: 3.10+
*   **CUDA**: 11.8+
*   **GPU**: æ¨ç†è‡³å°‘éœ€è¦ 16GB æ˜¾å­˜ï¼Œè®­ç»ƒæ¨èä½¿ç”¨ A100/H100ã€‚
</details>

### 1.2 å¯è§†åŒ–è¦æ±‚
å¦‚æœæ‚¨æƒ³åœ¨æ¨ç†å**å¯è§†åŒ–**ç»“æœã€è¿è¡Œ**æ•°æ®é›†å¯è§†åŒ–**ï¼Œæˆ–ä»å•å¼ å›¾åƒè¿›è¡Œé›¶æ ·æœ¬äººç±»æ‰‹éƒ¨åŠ¨ä½œé¢„æµ‹ï¼Œè¯·æŒ‰ç…§ä»¥ä¸‹è¯´æ˜æ“ä½œã€‚

**å®‰è£…å­æ¨¡å—**

è¯·å…‹éš†å­æ¨¡å—ä»¥è¿›è¡Œæ‰‹éƒ¨å§¿æ€ä¼°è®¡ã€‚
```bash
git submodule update --init --recursive
```

**å®‰è£…åº“**

è¯·ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å®‰è£…ç”¨äºå¯è§†åŒ–çš„é¢å¤–æ¨¡å—ï¼š

```bash
pip install -e .[visulization] --no-build-isolation
```

<details>
<summary>å¦‚æœåœ¨å®‰è£… <a href="https://github.com/facebookresearch/pytorch3d?tab=readme-ov-file">PyTorch3D</a> æ—¶é‡åˆ°é—®é¢˜ï¼Œè¯·ç‚¹å‡»è¿™é‡Œ </summary>

*   å¦‚æœæ‚¨åœ¨å®‰è£… [PyTorch3D](https://github.com/facebookresearch/pytorch3d?tab=readme-ov-file) æ—¶é‡åˆ°é—®é¢˜ï¼Œè¯·æŒ‰ç…§ [PyTorch3D](https://github.com/facebookresearch/pytorch3d?tab=readme-ov-file) ä»“åº“æä¾›çš„å®‰è£…è¯´æ˜è¿›è¡Œæ“ä½œï¼Œæˆ–è€…å°è¯•ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å•ç‹¬å®‰è£…ï¼š

    ```bash
    pip install --no-build-isolation git+https://github.com/facebookresearch/pytorch3d.git@stable#egg=pytorch3d
    ```
</details>


å¦‚æœæ‚¨çš„ç³»ç»Ÿæœªå®‰è£… [FFmpeg](https://github.com/FFmpeg/FFmpeg)ï¼Œè¯·å…ˆå®‰è£…å®ƒã€‚
```bash
sudo apt install ffmpeg
```

**MANO æ‰‹éƒ¨æ¨¡å‹**

æˆ‘ä»¬é‡å»ºçš„æ‰‹éƒ¨æ ‡ç­¾åŸºäº MANO æ‰‹éƒ¨æ¨¡å‹ã€‚**æˆ‘ä»¬åªéœ€è¦å³æ‰‹æ¨¡å‹ã€‚** æ¨¡å‹å‚æ•°å¯ä»¥ä» [å®˜æ–¹ç½‘ç«™](https://mano.is.tue.mpg.de/index.html) ä¸‹è½½ï¼Œå¹¶ç»„ç»‡æˆä»¥ä¸‹ç»“æ„ï¼š
```
weights/
â””â”€â”€ mano/
    â”œâ”€â”€ MANO_RIGHT.pkl
    â””â”€â”€ mano_mean_params.npz
```
è¯·ä¸‹è½½ [HaWoR](https://github.com/ThunderVVV/HaWoR) çš„æ¨¡å‹æƒé‡ç”¨äºæ‰‹éƒ¨å§¿æ€ä¼°è®¡ï¼š

```bash
wget https://huggingface.co/spaces/rolpotamias/WiLoR/resolve/main/pretrained_models/detector.pt -P ./weights/hawor/external/
wget https://huggingface.co/ThunderVVV/HaWoR/resolve/main/hawor/checkpoints/hawor.ckpt -P ./weights/hawor/checkpoints/
```

---

## 2. ä½¿ç”¨äººæ‰‹å›¾åƒè¿›è¡Œæ¨ç†
æ‚¨å¯ä»¥ä½¿ç”¨æˆ‘ä»¬çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œæ ¹æ®æŒ‡ä»¤ç›´æ¥ä»**ç¬¬ä¸€è§†è§’äººæ‰‹å›¾åƒï¼ˆæ¨ªå±ï¼‰**è¿›è¡Œé›¶æ ·æœ¬ 3D äººæ‰‹åŠ¨ä½œé¢„æµ‹ã€‚è¦ä»é¢„å…ˆæ•è·çš„å›¾åƒé¢„æµ‹äººç±»åŠ¨ä½œï¼Œè¯·è¿è¡Œ [`scripts/run_human_inference.sh`](scripts/run_human_inference.sh)ã€‚è¿™æ˜¯ä¸€ä¸ªç®€å•çš„ç¤ºä¾‹ï¼š
```bash
python scripts/inference_human_prediction.py \
    --config VITRA-VLA/VITRA-VLA-3B \
    --image_path ./examples/0002.jpg \
    --sample_times 4 \
    --save_state_local \
    --use_right \
    --video_path ./example_human_inf.mp4 \
    --mano_path ./weights/mano \
    --instruction "Left hand: None. Right hand: Pick up the picture of Michael Jackson." \
```
æ‰€æœ‰ç¤ºä¾‹å›¾åƒéƒ½æ˜¯åœ¨**V-L-A æ•°æ®é›†ä¸­ä»æœªå‡ºç°çš„æˆ¿é—´**å†…ä½¿ç”¨æ‰‹æœºæ‹æ‘„çš„ã€‚å®ƒä»¬è¿˜åŒ…å«äº†å®Œå…¨**æœªè§è¿‡çš„æ¦‚å¿µ**ï¼Œä¾‹å¦‚åäººçš„ç…§ç‰‡ã€‚

ç”¨æˆ·å¯ä»¥ä½¿ç”¨è‡ªå·±çš„è®¾å¤‡æ•è·å›¾åƒï¼Œå¹¶ç›´æ¥ä½¿ç”¨è®°å½•çš„å›¾åƒæµ‹è¯•æ¨¡å‹ã€‚
> **æ³¨æ„ï¼š**  
> ä¸ºäº†è·å¾—æœ€ä½³æ¨ç†è´¨é‡ï¼Œå»ºè®®æ•è·ä¸**äººå¤´**é«˜åº¦ç›¸è¿‘çš„**æ¨ªå±**è§†å›¾å›¾åƒï¼Œä»¥åŒ¹é…è‡ªç„¶çš„ç¬¬ä¸€è§†è§’è§‚å¯Ÿç‚¹ã€‚æåº¦å¼‚å¸¸æˆ–æ‰­æ›²çš„æ‰‹éƒ¨å§¿æ€/ä½ç½®å¯èƒ½ä¼šå¯¼è‡´æ¨ç†å¤±è´¥ã€‚


ä»¥ä¸‹æ˜¯é¢„æµ‹äººç±»åŠ¨ä½œçš„**æœ€å°åŒ–ä½¿ç”¨ç¤ºä¾‹**ã€‚


```python
import json
import torch
import numpy as np
from PIL import Image
from vitra.models import VITRA_Paligemma, load_model
from vitra.utils.data_utils import resize_short_side_to_target, load_normalizer
from vitra.datasets.human_dataset import pad_state_human, pad_action
from vitra.utils.config_utils import load_config
from vitra.datasets.dataset_utils import (
    ActionFeature,
    StateFeature,
)

# åŠ è½½é…ç½®
configs = load_config('VITRA-VLA/VITRA-VLA-3B')

# å¦‚æœæä¾›äº†è·¯å¾„åˆ™è¦†ç›–é…ç½®
pretrained_path = 'VITRA-VLA/VITRA-VLA-3B'
statistics_path = 'VITRA-VLA/VITRA-VLA-3B'
configs['model_load_path'] = pretrained_path
configs['statistics_path'] = statistics_path

# åŠ è½½æ¨¡å‹å’Œæ ‡å‡†åŒ–å™¨
model = load_model(configs).cuda()
model.eval()

normalizer = load_normalizer(configs)

image_path = "your_image.jpg"
image = Image.open(image_path)
image = resize_short_side_to_target(image, target=224)
fov = torch.tensor([[np.deg2rad(60.0), np.deg2rad(60.0)]])      # åœ¨æ­¤å¤„è¾“å…¥æ‚¨çš„ç›¸æœº FOV [fov_x, fov_y]

image = np.array(image)

# åœ¨æ­¤å¤„è¾“å…¥æ‚¨çš„æç¤ºè¯­ã€‚ä»¥é¢„æµ‹å³æ‰‹åŠ¨ä½œä¸ºä¾‹ã€‚
instruction = "Left hand: None. Right hand: Pick up the phone on the table."  

# åˆå§‹åŒ–çŠ¶æ€
# çŠ¶æ€å‘é‡ç»“æ„ (æ€»ç»´åº¦: 122):
#   - state_left [51]:      å·¦æ‰‹çŠ¶æ€å‘é‡
#       * [0:3]    transl:          ç›¸æœºç©ºé—´ä¸­çš„å¹³ç§» (x, y, zï¼Œå•ä½ä¸ºç±³)
#       * [3:6]    global_orient:   ä»¥æ¬§æ‹‰è§’è¡¨ç¤ºçš„å…¨å±€æ—‹è½¬ (xyzï¼Œå•ä½ä¸ºå¼§åº¦)
#       * [6:51]   hand_pose:       45 ä¸ªå…³èŠ‚è§’åº¦æ¬§æ‹‰è§’ (15 ä¸ªå…³èŠ‚ Ã— 3 è½´ï¼Œå•ä½ä¸ºå¼§åº¦)
#   - beta_left [10]:       å·¦æ‰‹ MANO å½¢çŠ¶å‚æ•°
#   - state_right [51]:     å³æ‰‹çŠ¶æ€å‘é‡ (ç»“æ„åŒ state_left)
#       * [0:3]    transl:          ç›¸æœºç©ºé—´ä¸­çš„å¹³ç§» (x, y, zï¼Œå•ä½ä¸ºç±³)
#       * [3:6]    global_orient:   ä»¥æ¬§æ‹‰è§’è¡¨ç¤ºçš„å…¨å±€æ—‹è½¬ (xyzï¼Œå•ä½ä¸ºå¼§åº¦)
#       * [6:51]   hand_pose:       45 ä¸ªå…³èŠ‚è§’åº¦æ¬§æ‹‰è§’ (15 ä¸ªå…³èŠ‚ Ã— 3 è½´ï¼Œå•ä½ä¸ºå¼§åº¦)
#   - beta_right [10]:      å³æ‰‹ MANO å½¢çŠ¶å‚æ•°
state = np.zeros((normalizer.state_mean.shape[0],))             # åœ¨æ­¤å¤„è¾“å…¥æ‚¨çš„æ‰‹éƒ¨çŠ¶æ€
# ä»…ä»¥ä½¿ç”¨å³æ‰‹çŠ¶æ€ä¸ºä¾‹ã€‚
# state_mask[0] æŒ‡ç¤ºæ˜¯å¦ä½¿ç”¨å·¦æ‰‹çŠ¶æ€ï¼Œ 
# state_mask[1] æŒ‡ç¤ºæ˜¯å¦ä½¿ç”¨å³æ‰‹çŠ¶æ€ã€‚
state_mask = np.array([False, True], dtype=bool)                # åœ¨æ­¤å¤„è¾“å…¥æ‚¨çš„æ‰‹éƒ¨çŠ¶æ€æ©ç ã€‚ 


# åœ¨æ­¤å¤„è¾“å…¥æ‚¨çš„ action_maskã€‚å½¢çŠ¶: (W, 2)ï¼Œå…¶ä¸­ W æ˜¯åˆ†å—å¤§å° (chunk_size)ã€‚ 
# action_mask[:, 0] æŒ‡ç¤ºæ˜¯å¦é¢„æµ‹å·¦æ‰‹åŠ¨ä½œï¼Œ 
# action_mask[:, 1] æŒ‡ç¤ºæ˜¯å¦é¢„æµ‹å³æ‰‹åŠ¨ä½œã€‚ 
# ç¤ºä¾‹ä¸­å·¦æ‰‹å…¨ä¸º Falseï¼Œå³æ‰‹å…¨ä¸º Trueã€‚
action_mask = np.tile(np.array([[False, True]], dtype=bool), (model.chunk_size, 1))  


# æ ‡å‡†åŒ–çŠ¶æ€
norm_state = normalizer.normalize_state(state)

unified_action_dim = ActionFeature.ALL_FEATURES[1]   # 192
unified_state_dim = StateFeature.ALL_FEATURES[1]     # 212

unified_state, unified_state_mask = pad_state_human(
    state = norm_state,
    state_mask = state_mask,
    action_dim = normalizer.action_mean.shape[0],
    state_dim = normalizer.state_mean.shape[0],
    unified_state_dim = unified_state_dim,
)
_, unified_action_mask = pad_action(
    actions=None,
    action_mask=action_mask,
    action_dim=normalizer.action_mean.shape[0],
    unified_action_dim=unified_action_dim
)

# æ¨¡å‹æ¨ç†
norm_action = model.predict_action(
    image = image,
    instruction = instruction,
    current_state = unified_state.unsqueeze(0),
    current_state_mask = unified_state_mask.unsqueeze(0),
    action_mask_torch = unified_action_mask.unsqueeze(0),
    num_ddim_steps = 10,
    cfg_scale = 5.0,
    fov = fov,
    sample_times = 1
)
norm_action = norm_action[0, :,:102]
# åæ ‡å‡†åŒ–é¢„æµ‹åŠ¨ä½œ
unnorm_action = normalizer.unnormalize_action(norm_action)
print("é¢„æµ‹åŠ¨ä½œ:", unnorm_action)
```

---

## 3. ä½¿ç”¨è‡ªå®šä¹‰æœºå™¨äººæ•°æ®é›†è¿›è¡Œå¾®è°ƒ


æˆ‘ä»¬çš„ VITRA æ¨¡å‹å¯ä½œä¸ºç‰¹å®šæœºå™¨äººå¾®è°ƒçš„èµ·ç‚¹ï¼ˆä¾‹å¦‚åœ¨ Xhand æˆ–æ‚¨çš„è‡ªå®šä¹‰æœºå™¨äººä¸Šï¼‰ã€‚

### 3.1 æ•°æ®å‡†å¤‡

æˆ‘ä»¬ä½¿ç”¨**ç›¸æœºç©ºé—´æœ«ç«¯æ‰§è¡Œå™¨ (EEF) å§¿æ€**æ¥ä»£è¡¨äººæ‰‹çš„è…•éƒ¨å§¿æ€ã€‚å‘é€ç»™ **XHand çš„é¥æ“ä½œå‘½ä»¤**è¢«è§†ä¸ºçµå·§æ‰‹**å…³èŠ‚åŠ¨ä½œ**ï¼Œæ£€ç´¢åˆ°çš„ **XHand å…³èŠ‚è§’åº¦**è¢«ç”¨ä½œæ‰‹å…³èŠ‚çŠ¶æ€ã€‚åœ¨è¿›è¡Œæœºå™¨äººæ•°æ®å¾®è°ƒä¹‹å‰ï¼Œæˆ‘ä»¬å¼ºçƒˆå»ºè®®å°†åŒæ‰‹çš„ EEF **å¹³ç§»**å’Œ**æ—‹è½¬**åæ ‡ç³»ä¸äººæ‰‹ V-L-A æ•°æ®é›†ä¸­ä½¿ç”¨çš„åæ ‡ç³»å¯¹é½ã€‚

å°†æ‰€æœ‰åæ ‡å¯¹é½åˆ°**ç›¸æœºåæ ‡ç³»**ï¼š

- **X è½´**: æŒ‡å‘å±å¹•å³ä¾§ï¼ˆæ­£æ–¹å‘ï¼‰
- **Y è½´**: æŒ‡å‘å±å¹•ä¸‹æ–¹ï¼ˆæ­£æ–¹å‘ï¼‰
- **Z è½´**: æŒ‡å‘è¿œç¦»ç›¸æœºçš„æ–¹å‘ï¼Œå‚ç›´è¿›å…¥å±å¹•ï¼ˆæ­£æ–¹å‘ï¼‰

å¯¹äº **EEF æ—‹è½¬åŸç‚¹**ï¼Œè¯·æ³¨æ„å·¦å³æ‰‹ä¹‹é—´çš„é•œåƒå…³ç³»ã€‚å¯¹é½æ—‹è½¬åŸç‚¹åï¼ŒEEF å§¿æ€åº”ä¸ä¸‹å›¾ä¸­çš„æƒ…å†µåŒ¹é…ã€‚å·¦ä¾§æ˜¯äººæ‰‹ V-L-A æ•°æ®é›†çš„ç¤ºä¾‹ï¼›å³ä¾§æ˜¯ XHand æ•°æ®é›†å¯¹é½åçš„ç¤ºä¾‹ã€‚

![coordinate_alignment](assets/coordinate_alignment.png)

æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨ [`vitra/dataset/robot_dataset.py`](./vitra/dataset/robot_dataset.py) ä¸­æä¾›äº† `transfer_xhand_to_human` å‡½æ•°ï¼Œç”±äºè¯¥å‡½æ•°ä¼šå°† XHand å…³èŠ‚è§’åº¦æ˜ å°„åˆ°äººæ‰‹è¡¨ç¤ºä¸­æœ€æ¥è¿‘çš„è‡ªç”±åº¦ã€‚ï¼ˆæ³¨æ„ï¼Œè¿™ç§ XHand åˆ°äººæ‰‹çš„å¯¹é½**ä¸éœ€è¦ç¦»çº¿é¢„å¤„ç†**ã€‚å®ƒå°†åœ¨ `RoboDatasetCore` ç±»çš„ `transform_trajectory` å‡½æ•°å†…éƒ¨è¿›è¡Œè½¨è¿¹è½¬æ¢æ—¶è‡ªåŠ¨åº”ç”¨ã€‚ï¼‰

å¦‚æœæ‚¨ä½¿ç”¨çš„æ˜¯ä¸åŒçš„çµå·§æ‰‹æ¨¡å‹ï¼Œæˆ‘ä»¬å»ºè®®å®ç°ç±»ä¼¼çš„å‡½æ•°ï¼Œå°†å…¶å…³èŠ‚é…ç½®ä¸äººæ‰‹è‡ªç”±åº¦å¯¹é½ã€‚


### 3.2 å®ç°è‡ªå®šä¹‰ `RoboDatasetCore`

è¦ä½¿æ•°æ®é›†åŠ è½½å™¨é€‚é…æ‚¨è‡ªå·±çš„æœºå™¨äººæ•°æ®ï¼Œæ‚¨åº”è¯¥åœ¨ [`vitra/dataset/robot_dataset.py`](./vitra/dataset/robot_dataset.py) ä¸­åˆ›å»º `RoboDatasetCore` ç±»çš„è‡ªå®šä¹‰å®ç°ã€‚

é€šå¸¸éœ€è¦é‡å†™ä»¥ä¸‹æ–¹æ³•ä»¥åŒ¹é…**äººç±»æ•°æ®é¢„è®­ç»ƒæ ¼å¼**ï¼š

```python
def __init__(...):
    # æ•°æ®é›†åˆå§‹åŒ–
def __len__(self):
    # è¿”å›å¸§æ•°
    ...

def __getitem__(self, idx):
    """
    è¿”å›æ ·æœ¬å­—å…¸
    """
    ...
    return {
        "instruction": instruction,
        "image_list": image_list,
        "image_mask": image_mask,
        "action_list": action_list,
        "action_mask": action_mask,
        "current_state": current_state,
        "current_state_mask": current_state_mask,
        "fov": fov,
        "intrinsics": self.intrinsics,
    }
```

`__getitem__` è¿”å›çš„å­—å…¸åŒ…å«ä»£è¡¨æœºå™¨äººçŠ¶æ€ã€åŠ¨ä½œå’Œè§‚æµ‹çš„å¤šä¸ªå­—æ®µã€‚è¿™é‡Œæˆ‘ä»¬ä»¥ **XHand** é…ç½®ä¸ºä¾‹è¿›è¡Œè¯´æ˜ï¼š

- **å·¦æ‰‹**: 6-DoF EEF å§¿æ€ + 12 å…³èŠ‚è§’åº¦ = 18-DoF
- **å³æ‰‹**: 6-DoF EEF å§¿æ€ + 12 å…³èŠ‚è§’åº¦ = 18-DoF
- **çŠ¶æ€æˆ–åŠ¨ä½œçš„ç»´åº¦æ’åº**: [`left_eef_trans`, `left_eef_euler_rotation`, `left_hand_joint`, `right_eef_trans`, `right_eef_euler_rotation`, `right_hand_joint`]

</br>

| é”® (Key)                | ç±»å‹                     | å½¢çŠ¶ (Shape)      | æè¿°                                                                                               |
|------------------------|--------------------------|-------------------|-----------------------------------------------------------------------------------------------------------|
| **instruction**        | `str`                    | â€”                 | ä»»åŠ¡çš„è‡ªç„¶è¯­è¨€æè¿°ã€‚<br> ä¾‹å¦‚ï¼š`"Left hand: None. Right hand: {Right hand prompt}"` |
| **image_list**         | `np.ndarray (uint8)`     | `(1, H, W, C)`    | RGB å›¾åƒåºåˆ—ã€‚<br>â€¢ `C=3` (é€šé“) <br>â€¢ `H, W` = é«˜åº¦å’Œå®½åº¦                                  |
| **image_mask**         | `np.ndarray (bool)`      | `(1,)`            | æŒ‡ç¤ºå“ªäº›å¸§æ˜¯æœ‰æ•ˆçš„ï¼ˆ`1=æœ‰æ•ˆ`, `0=å¡«å……`ï¼‰ã€‚                                                 |
| **action_list**        | `np.ndarray (float32)`   | `(T, 36)`         | æœºå™¨äººåŠ¨ä½œåºåˆ—ï¼ˆåŠ¨ä½œåˆ†å—/action chunkingï¼‰ã€‚<br>â€¢ `T` = åŠ¨ä½œåˆ†å—çš„é•¿åº¦                                                         |
| **action_mask**        | `np.ndarray (bool)`      | `(T, 2)`          | æŒ‡ç¤ºå“ªäº›æ—¶é—´æ­¥åŒ…å«æœ‰æ•ˆçš„å·¦æ‰‹æˆ–å³æ‰‹åŠ¨ä½œã€‚`action_mask[:, 0]` å¯¹åº”å·¦æ‰‹ï¼Œ`action_mask[:, 1]` å¯¹åº”å³æ‰‹ã€‚                                                      |
| **current_state**      | `np.ndarray (float32)`   | `(36,)`           | å½“å‰æ—¶é—´æ­¥çš„æœºå™¨äººçŠ¶æ€ã€‚                                                                     |
| **current_state_mask** | `np.ndarray (float32)`   | `(2,)`            | æŒ‡ç¤ºå“ªåªæ‰‹çš„çŠ¶æ€æ˜¯æœ‰æ•ˆçš„ã€‚`current_state_mask[0]` å¯¹åº”å·¦æ‰‹ï¼Œ`current_state_mask[1]` å¯¹åº”å³æ‰‹ã€‚                                           |
| **fov**                | `np.ndarray (float32)`   | `(2,)`            | ç›¸æœºè§†åœºè§’ã€‚`[fov_x, fov_y]`                                                                                    |
| **intrinsics**         | `np.ndarray (float32)`   | `(3, 3)`          | ç›¸æœºå†…å‚çŸ©é˜µã€‚åœ¨è®­ç»ƒæˆ–æ¨¡å‹æ¨ç†æœŸé—´ä¸ä½¿ç”¨ã€‚                                     |




### 3.3 è®¡ç®—æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯

åœ¨è®­ç»ƒä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦è®¡ç®—æœºå™¨äººæ•°æ®é›†çš„**æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯**ï¼Œç‰¹åˆ«æ˜¯**çŠ¶æ€**å’Œ**åŠ¨ä½œ**å˜é‡çš„å‡å€¼å’Œæ ‡å‡†å·®ã€‚è¿™äº›ç»Ÿè®¡æ•°æ®å°†ç”¨äºä½¿ç”¨é«˜æ–¯æ ‡å‡†åŒ–æ¥**æ ‡å‡†åŒ–åŠ¨ä½œ**ã€‚

### 3.4 ä¿®æ”¹é…ç½®

å‡†å¤‡å¥½æ•°æ®é›†å¹¶è®¡ç®—ç»Ÿè®¡ä¿¡æ¯åï¼Œæ‚¨å¯ä»¥ç¼–è¾‘ `vitra/configs/robot_finetune.json` æ–‡ä»¶æ¥æ›´æ–°è·¯å¾„å’Œå…¶ä»–ç›¸å…³è®¾ç½®ã€‚`pretrain_path` åº”æ›¿æ¢ä¸ºé¢„è®­ç»ƒæ¨¡å‹çš„è·¯å¾„æˆ–åŒ…å«å®ƒçš„æ–‡ä»¶å¤¹ï¼Œä»¥ä¾¿æ­£ç¡®æ‰§è¡Œå¾®è°ƒã€‚

åœ¨ `scripts/run_robot_finetune.sh` ä¸­ï¼Œç¡®ä¿è¾“å…¥æ‚¨çš„ Hugging Face token å’Œ WANDB API keyï¼Œä»¥ä¾¿è¿›è¡Œèº«ä»½éªŒè¯å’Œæ—¥å¿—è®°å½•ã€‚

### 3.5 è¿è¡Œè„šæœ¬

é…ç½®å®Œæˆåï¼Œæ‚¨å¯ä»¥é€šè¿‡è¿è¡Œä»¥ä¸‹å‘½ä»¤å¼€å§‹å¾®è°ƒï¼š

```bash
bash scripts/run_robot_finetune.sh
```

<!-- TODO: æä¾›ç¤ºä¾‹ä»£ç  -->
---

## 4. å®ç‰©éƒ¨ç½²
ä»¥ä¸‹æ˜¯åœ¨ XHand å¹³å°ä¸Šè¿è¡Œ **VITRA** æ¨ç†çš„ç¤ºä¾‹ã€‚

```python
import json
import torch
import numpy as np
from PIL import Image
from vitra.models import VITRA_Paligemma, load_model
from vitra.utils.data_utils import resize_short_side_to_target
from vitra.datasets.human_dataset import pad_state_human, pad_action
from vitra.utils.data_utils import load_normalizer
from vitra.datasets.dataset_utils import (
    ActionFeature,
    StateFeature,
)
from vitra.datasets.robot_dataset import (
    transfer_xhand_to_human,
    transfer_human_to_xhand,
    pad_state_robot, pad_action
)
# åŠ è½½é…ç½®
configs = json.load(open('configs/robot_finetune.json'))
pretrained_path = 'checkpoints/finetuned_model.pt'
statistics_path = 'statistics/RoboData_statistics.json'
configs['model_load_path'] = pretrained_path
configs['statistics_path'] = statistics_path

# åŠ è½½æ¨¡å‹å’Œæ ‡å‡†åŒ–å™¨
model = load_model(configs).cuda()
model.eval()

normalizer = load_normalizer(configs)

image_path = "your_image.jpg"
image = Image.open(image_path)
image = resize_short_side_to_target(image, target=224)
fov = torch.tensor([[np.deg2rad(60.0), np.deg2rad(60.0)]])      # åœ¨æ­¤å¤„è¾“å…¥æ‚¨çš„ç›¸æœº FOV [fov_x, fov_y]

image = np.array(image)

# åœ¨æ­¤å¤„è¾“å…¥æ‚¨çš„æç¤ºè¯­ã€‚ä»¥é¢„æµ‹å³æ‰‹åŠ¨ä½œä¸ºä¾‹ã€‚
instruction = "Left hand: None. Right hand: Pour the contents of the bottle into the pot."  

# åˆå§‹åŒ–çŠ¶æ€
# çŠ¶æ€å‘é‡ç»“æ„ (æ€»ç»´åº¦: 36):
#   - å·¦æ‰‹ [0:18]:
#       * [0:3]    transl:          ç›¸æœºç©ºé—´ä¸­çš„è…•éƒ¨å¹³ç§» (x, y, zï¼Œå•ä½ä¸ºç±³)
#       * [3:6]    global_orient:   ä»¥æ¬§æ‹‰è§’è¡¨ç¤ºçš„è…•éƒ¨æ—‹è½¬ (xyzï¼Œå•ä½ä¸ºå¼§åº¦)
#       * [6:18]   hand_pose:       XHand å…³èŠ‚è§’åº¦ (12 ä¸ªå…³èŠ‚ï¼Œå•ä½ä¸ºå¼§åº¦)
#   - å³æ‰‹ [18:36]:
#       * [18:21]  transl:          ç›¸æœºç©ºé—´ä¸­çš„è…•éƒ¨å¹³ç§» (x, y, zï¼Œå•ä½ä¸ºç±³)
#       * [21:24]  global_orient:   ä»¥æ¬§æ‹‰è§’è¡¨ç¤ºçš„è…•éƒ¨æ—‹è½¬ (xyzï¼Œå•ä½ä¸ºå¼§åº¦)
#       * [24:36]  hand_pose:       XHand å…³èŠ‚è§’åº¦ (12 ä¸ªå…³èŠ‚ï¼Œå•ä½ä¸ºå¼§åº¦)
state = np.zeros((normalizer.state_mean.shape[0],))             # åœ¨æ­¤å¤„è¾“å…¥æ‚¨çš„æ‰‹éƒ¨çŠ¶æ€
# ä»…ä»¥ä½¿ç”¨å³æ‰‹çŠ¶æ€ä¸ºä¾‹ã€‚
# state_mask[0] æŒ‡ç¤ºæ˜¯å¦ä½¿ç”¨å·¦æ‰‹çŠ¶æ€ï¼Œ 
# state_mask[1] æŒ‡ç¤ºæ˜¯å¦ä½¿ç”¨å³æ‰‹çŠ¶æ€ã€‚
state_mask = np.array([False, True], dtype=bool)                # åœ¨æ­¤å¤„è¾“å…¥æ‚¨çš„æ‰‹éƒ¨çŠ¶æ€æ©ç ã€‚ 


# åœ¨æ­¤å¤„è¾“å…¥æ‚¨çš„ action_maskã€‚å½¢çŠ¶: (W, 2)ï¼Œå…¶ä¸­ W æ˜¯ chunk_sizeã€‚ 
# action_mask[:, 0] æŒ‡ç¤ºæ˜¯å¦é¢„æµ‹å·¦æ‰‹åŠ¨ä½œï¼Œ 
# action_mask[:, 1] æŒ‡ç¤ºæ˜¯å¦é¢„æµ‹å³æ‰‹åŠ¨ä½œã€‚ 
# ç¤ºä¾‹ä¸­å·¦æ‰‹å…¨ä¸º Falseï¼Œå³æ‰‹å…¨ä¸º Trueã€‚
action_mask = np.tile(np.array([[False, True]], dtype=bool), (model.chunk_size, 1))  


# æ ‡å‡†åŒ–çŠ¶æ€
norm_state = normalizer.normalize_state(state)

unified_action_dim = ActionFeature.ALL_FEATURES[1]   # 192
unified_state_dim = StateFeature.ALL_FEATURES[1]     # 212

unified_state, unified_state_mask = pad_state_robot(
    state = norm_state,
    state_mask = state_mask,
    state_dim = normalizer.state_mean.shape[0],
    unified_state_dim = unified_state_dim,
)
_, unified_action_mask = pad_action(
    actions=None,
    action_mask=action_mask,
    action_dim=normalizer.action_mean.shape[0],
    unified_action_dim=unified_action_dim
)
human_state, human_state_mask, _, human_action_mask = transfer_xhand_to_human(
    unified_state, unified_state_mask,
    None, unified_action_mask
)
# æ¨¡å‹æ¨ç†
norm_action = model.predict_action(
    image = image,
    instruction = instruction,
    current_state = human_state.unsqueeze(0),
    current_state_mask = human_state_mask.unsqueeze(0),
    action_mask_torch = human_action_mask.unsqueeze(0),
    num_ddim_steps = 10,
    cfg_scale = 5.0,
    fov = fov,
    sample_times = 1
)
norm_action = norm_action[0, :,:102]
norm_robot_action = transfer_human_to_xhand(norm_action)
# åæ ‡å‡†åŒ–é¢„æµ‹åŠ¨ä½œ
unnorm_action = normalizer.unnormalize_action(norm_robot_action)
print("é¢„æµ‹åŠ¨ä½œå½¢çŠ¶:", unnorm_action.shape)
# ç»“æœä¸º 2 ä¸ª 18-DoF åŠ¨ä½œï¼Œå…± 16 æ­¥ï¼Œå½¢çŠ¶ä¸º [16, 36]
```

---

## 5. äººæ‰‹ VLA æ•°æ®é›†åˆ©ç”¨


æˆ‘ä»¬å‘å¸ƒäº† **Human Hand V-L-A** æ•°æ®é›†ï¼Œå®ƒå°†â€œé‡å¤–â€è§†é¢‘è½¬æ¢ä¸ºæœºå™¨äººå¯¹é½çš„ `(å›¾åƒ, æŒ‡ä»¤, åŠ¨ä½œ)` å…ƒç»„ã€‚

æ‚¨å¯ä»¥ä» [VITRA-1M](https://huggingface.co/datasets/VITRA-VLA/VITRA-1M) ä¸‹è½½æ•°æ®é›†æ ‡æ³¨ã€‚
ä¸‹è½½åï¼Œè¯·ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤è§£å‹æ‰€æœ‰ `.gz` æ–‡ä»¶ï¼š
```bash
tar -xzvf ego4d_cooking_and_cleaning.tar.gz
tar -xzvf ego4d_other.tar.gz
tar -xzvf egoexo4d.tar.gz
tar -xzvf ssv2.tar.gz
tar -xzvf epic.tar.gz
```
æœ‰å…³æ•°æ®é›†ã€å…¶ç»“æ„å’Œä½¿ç”¨è¯´æ˜çš„è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è€ƒ [`data/data.md`](data/data.md) æ–‡ä»¶ã€‚


----


## 6. ä»å¤´å¼€å§‹è¿›è¡Œäººç±»æ•°æ®é¢„è®­ç»ƒ

è¦åœ¨ **Human-VLA** æ•°æ®é›†ä¸Šé‡ç°æˆ‘ä»¬çš„é¢„è®­ç»ƒç»“æœï¼š

### 6.1 æ•°æ®é›†å‡†å¤‡

é¦–å…ˆï¼Œè¯·æŒ‰ç…§ [`data/data.md`](data/data.md) ä¸­çš„è¯´æ˜ä¸‹è½½è§†é¢‘æ•°æ®é›†å’Œç›¸åº”çš„æ ‡æ³¨æ–‡ä»¶ã€‚ä¸‹è½½åï¼Œåœ¨è¿›ä¸€æ­¥é¢„å¤„ç†ä¹‹å‰å¯¹è§†é¢‘è¿›è¡Œ **[å»ç•¸å˜ (undistortion)](data/data.md)**ï¼Œä»¥æ ¡æ­£é±¼çœ¼å’Œé•œå¤´ç•¸å˜ã€‚

#### å…³äº EgoExo4D è§†é¢‘é¢„å¤„ç†çš„æ³¨æ„äº‹é¡¹

å»ç•¸å˜åçš„ EgoExo4D è§†é¢‘ç”±äºé±¼çœ¼æ ¡æ­£è€ŒåŒ…å«é»‘è¾¹ã€‚æˆ‘ä»¬åº”ç”¨äº†é¢å¤–çš„é¢„å¤„ç†æ­¥éª¤æ¥ç§»é™¤è¿™äº›é»‘è¾¹ï¼šé¦–å…ˆå°†æ‰€æœ‰ EgoExo4D å¸§å¤§å°è°ƒæ•´ä¸º 448Ã—448ï¼Œç„¶åä¸­å¿ƒè£å‰ªä¸º 256Ã—256ã€‚å†…å‚çš„å˜åŒ–å·²åœ¨ `vitra/datasets/human_dataset.py` ä¸­å¤„ç†ï¼Œå…¶ä¸­ç›¸æœºå†…å‚çš„è®¡ç®—åŒ…å«äº†è£å‰ªå˜æ¢å¹¶ç›¸åº”åœ°è°ƒæ•´äº†ç›¸æœº FOVã€‚


å‡†å¤‡å¥½åï¼ŒæŒ‰ä»¥ä¸‹ç›®å½•ç»“æ„ç»„ç»‡æ•°æ®ï¼š
```
Data_root/
â”œâ”€â”€ Video/
â”‚   â”œâ”€â”€ Ego4D_root/
â”‚   â”‚   â”œâ”€â”€ {è§†é¢‘åç§°1}.mp4
â”‚   â”‚   â”œâ”€â”€ {è§†é¢‘åç§°2}.mp4
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ Epic-Kitchen_root/
â”‚   â”‚   â”œâ”€â”€ {è§†é¢‘åç§°1}.MP4
â”‚   â”‚   â”œâ”€â”€ {è§†é¢‘åç§°2}.MP4
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ EgoExo4D_root/
â”‚   â”‚   â”œâ”€â”€ {è§†é¢‘åç§°1}.mp4
â”‚   â”‚   â”œâ”€â”€ {è§†é¢‘åç§°2}.mp4
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ Somethingsomething-v2_root/
â”‚       â”œâ”€â”€ {è§†é¢‘åç§°1}.webm
â”‚       â”œâ”€â”€ {è§†é¢‘åç§°2}.webm
â”‚       â””â”€â”€ ...
â””â”€â”€ Annotation/
    â”œâ”€â”€ ego4d_cooking_and_cleaning/
    â”œâ”€â”€ ego4d_other/
    â”œâ”€â”€ egoexo4d/
    â”œâ”€â”€ epic/
    â”œâ”€â”€ ssv2/
    â””â”€â”€ statistics/
```

#### æ³¨æ„ï¼š

- `Video/` åŒ…å«æ¥è‡ªä¸åŒæ¥æºï¼ˆå¦‚ Ego4Dã€Epic-Kitchenã€EgoExo4D å’Œ Something-Something v2ï¼‰çš„æ‰€æœ‰åŸå§‹è§†é¢‘æ–‡ä»¶ã€‚
- `Annotation/` åŒ…å«ä¸ç›¸åº”è§†é¢‘å¯¹é½çš„æ ‡æ³¨æ–‡ä»¶ã€‚

#### (å¯é€‰) è®­ç»ƒåŠ é€ŸæŠ€å·§

ä¸ºäº†è¿›ä¸€æ­¥åŠ é€Ÿè®­ç»ƒï¼Œæˆ‘ä»¬å»ºè®®æ‰§è¡Œä»¥ä¸‹å¯é€‰çš„é¢„å¤„ç†æ­¥éª¤ï¼š

1. è°ƒæ•´æ‰€æœ‰å¤„ç†åçš„è§†é¢‘å¤§å°ï¼Œä½¿çŸ­è¾¹ä¸º 224ï¼Œä»¥å®ç°æ›´å¿«çš„è§£ç å¹¶å‡å°‘å†…å­˜å ç”¨ï¼ˆåœ¨è°ƒæ•´å¤§å°æ—¶ä¿æŒçºµæ¨ªæ¯”ï¼‰ã€‚
2. å°†é•¿è§†é¢‘åˆ†å‰²æˆæ¯ä¸ªæœ€å¤š 2000 å¸§çš„çŸ­ç‰‡æ®µã€‚
   ä½¿ç”¨ä»¥ä¸‹æ ¼å¼å‘½åç‰‡æ®µï¼š`{video_name}_part{part_index}.mp4`ï¼Œå…¶ä¸­ `part_index` ä» **1** å¼€å§‹ã€‚
3. åœ¨ `configs/human_pretrain.json` ä¸­è®¾ç½®ï¼š
```json
"clip_len": 2000
```
è¿™æ˜¾è‘—æé«˜äº†è®­ç»ƒæœŸé—´çš„æ•°æ®åŠ è½½é€Ÿåº¦ï¼Œç‰¹åˆ«æ˜¯å¯¹äºå¤§è§„æ¨¡æ•°æ®é›†ã€‚

### 6.2 è®¡ç®—æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯

æ‚¨å¯ä»¥ç›´æ¥ä½¿ç”¨ä¸‹è½½çš„æ•°æ®é›†ä¸­æä¾›çš„é¢„è®¡ç®—ç»Ÿè®¡ä¿¡æ¯ã€‚
æˆ–è€…ï¼Œæ‚¨å¯ä»¥é€šè¿‡è¿è¡Œä»¥ä¸‹å‘½ä»¤æ¥è®¡ç®—æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯ï¼ˆçŠ¶æ€å’ŒåŠ¨ä½œçš„å‡å€¼å’Œæ ‡å‡†å·®ï¼‰ã€‚

```bash
python vitra/datasets/calculate_statistics.py --save_folder data_root/statistics
```

### 6.3 ä¿®æ”¹é…ç½®

ç¼–è¾‘ `vitra/configs/human_pretrain.json` æ–‡ä»¶ä»¥æ›´æ–°è·¯å¾„å’Œå…¶ä»–ç›¸å…³è®¾ç½®ã€‚

åœ¨ `scripts/run_human_pretrain.sh` ä¸­ï¼Œç¡®ä¿è¾“å…¥æ‚¨çš„ Hugging Face token å’Œ WANDB API keyï¼Œä»¥ä¾¿è¿›è¡Œèº«ä»½éªŒè¯å’Œæ—¥å¿—è®°å½•ã€‚

### 6.4 è¿è¡Œè„šæœ¬

é…ç½®å®Œæˆåï¼Œæ‚¨å¯ä»¥é€šè¿‡è¿è¡Œä»¥ä¸‹å‘½ä»¤å¼€å§‹é¢„è®­ç»ƒï¼š
```bash
# åˆ†å¸ƒå¼è®­ç»ƒ
bash scripts/run_human_pretrain.sh
```


---

## å¼•ç”¨

å¦‚æœæ‚¨å‘ç°æˆ‘ä»¬çš„å·¥ä½œåœ¨æ‚¨çš„ç ”ç©¶ä¸­æœ‰ç”¨ï¼Œè¯·å¼•ç”¨ï¼š

```bibtex
@article{li2025vitra,
  title={Scalable Vision-Language-Action Model Pretraining for Robotic Manipulation with Real-Life Human Activity Videos},
  author={Qixiu Li and Yu Deng and Yaobo Liang and Lin Luo and Lei Zhou and Chengtang Yao and Lingqi Zeng and Zhiyuan Feng and Huizhi Liang and Sicheng Xu and Yizhong Zhang and Xi Chen and Hao Chen and Lily Sun and Dong Chen and Jiaolong Yang and Baining Guo},
  journal={arXiv preprint arXiv:2510.21571},
  year={2025}
}
```
