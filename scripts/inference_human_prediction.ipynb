{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Hand VLA Inference and Visualization\n",
                "\n",
                "This notebook provides an interactive interface for hand action prediction and visualization using the VITRA model."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
                "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
                "\n",
                "import sys\n",
                "import cv2\n",
                "import math\n",
                "import json\n",
                "import torch\n",
                "import numpy as np\n",
                "from PIL import Image, ImageOps\n",
                "from pathlib import Path\n",
                "import multiprocessing as mp\n",
                "from scipy.spatial.transform import Rotation as R\n",
                "\n",
                "# Add project root to sys.path\n",
                "# Assuming the notebook is in scripts/\n",
                "repo_root = Path(os.getcwd()).parent\n",
                "if str(repo_root) not in sys.path:\n",
                "    sys.path.insert(0, str(repo_root))\n",
                "\n",
                "from vitra.models import VITRA_Paligemma, load_model\n",
                "from vitra.utils.data_utils import resize_short_side_to_target, load_normalizer, recon_traj\n",
                "from vitra.utils.config_utils import load_config\n",
                "from vitra.datasets.human_dataset import pad_state_human, pad_action\n",
                "from vitra.datasets.dataset_utils import (\n",
                "    compute_new_intrinsics_resize, \n",
                "    calculate_fov,\n",
                "    ActionFeature,\n",
                "    StateFeature,\n",
                ")\n",
                "\n",
                "from visualization.visualize_core import HandVisualizer, normalize_camera_intrinsics, save_to_video, Renderer, process_single_hand_labels\n",
                "from visualization.visualize_core import Config as HandConfig"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Helper Functions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_state(hand_data, hand_side='right'):\n",
                "    \"\"\"\n",
                "    Load and extract hand state from hand data.\n",
                "    \"\"\"\n",
                "    if hand_side not in ['left', 'right']:\n",
                "        raise ValueError(f\"hand_side must be 'left' or 'right', got '{hand_side}'\")\n",
                "    \n",
                "    hand_pose_t0 = hand_data[hand_side][0]['hand_pose']\n",
                "    hand_pose_t0_euler = R.from_matrix(hand_pose_t0).as_euler('xyz', degrees=False) # [15, 3]\n",
                "    hand_pose_t0_euler = hand_pose_t0_euler.reshape(-1)  # [45]\n",
                "    global_orient_mat_t0 = hand_data[hand_side][0]['global_orient']\n",
                "    R_t0_euler = R.from_matrix(global_orient_mat_t0).as_euler('xyz', degrees=False)  # [3]\n",
                "    transl_t0 = hand_data[hand_side][0]['transl']  # [3]\n",
                "    state_t0 = np.concatenate([transl_t0, R_t0_euler, hand_pose_t0_euler])  # [3+3+45=51]\n",
                "    fov_x = hand_data['fov_x']\n",
                "\n",
                "    return state_t0, hand_data[hand_side][0]['beta'], fov_x, None\n",
                "\n",
                "def euler_traj_to_rotmat_traj(euler_traj, T):\n",
                "    \"\"\"\n",
                "    Convert Euler angle trajectory to rotation matrix trajectory.\n",
                "    \"\"\"\n",
                "    hand_pose = euler_traj.reshape(-1, 3)  # [T*15, 3]\n",
                "    pose_matrices = R.from_euler('xyz', hand_pose).as_matrix()  # [T*15, 3, 3]\n",
                "    pose_matrices = pose_matrices.reshape(T, 15, 3, 3)  # [T, 15, 3, 3]\n",
                "\n",
                "    return pose_matrices"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Persistent Workers for Multiprocessing\n",
                "\n",
                "These functions run in separate processes to avoid CUDA context conflicts."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def _hand_reconstruction_worker(args_dict, task_queue, result_queue):\n",
                "    \"\"\"\n",
                "    Persistent worker for hand reconstruction that runs in a separate process.\n",
                "    \"\"\"\n",
                "    from data.tools.hand_recon_core import Config, HandReconstructor\n",
                "    \n",
                "    hand_reconstructor = None\n",
                "    \n",
                "    try:\n",
                "        class ArgsObj:\n",
                "            pass\n",
                "        args_obj = ArgsObj()\n",
                "        for key, value in args_dict.items():\n",
                "            setattr(args_obj, key, value)\n",
                "        \n",
                "        print(\"[HandRecon Process] Initializing hand reconstructor...\")\n",
                "        config = Config(args_obj)\n",
                "        hand_reconstructor = HandReconstructor(config=config, device='cuda')\n",
                "        print(\"[HandRecon Process] Hand reconstructor ready\")\n",
                "        \n",
                "        result_queue.put({'type': 'ready'})\n",
                "        \n",
                "        while True:\n",
                "            task = task_queue.get()\n",
                "            if task['type'] == 'shutdown':\n",
                "                break\n",
                "            elif task['type'] == 'reconstruct':\n",
                "                try:\n",
                "                    image_path = task['image_path']\n",
                "                    image = cv2.imread(image_path)\n",
                "                    if image is None:\n",
                "                        raise ValueError(f\"Failed to load image from {image_path}\")\n",
                "                    \n",
                "                    image_list = [image]\n",
                "                    recon_results = hand_reconstructor.recon(image_list)\n",
                "                    result_queue.put({'type': 'result', 'success': True, 'data': recon_results})\n",
                "                except Exception as e:\n",
                "                    import traceback\n",
                "                    result_queue.put({'type': 'result', 'success': False, 'error': str(e), 'traceback': traceback.format_exc()})\n",
                "    except Exception as e:\n",
                "        import traceback\n",
                "        result_queue.put({'type': 'error', 'error': str(e), 'traceback': traceback.format_exc()})\n",
                "    finally:\n",
                "        if hand_reconstructor is not None:\n",
                "            del hand_reconstructor\n",
                "            torch.cuda.empty_cache()\n",
                "            torch.cuda.synchronize()\n",
                "        print(\"[HandRecon Process] Exiting\")\n",
                "\n",
                "def _vla_inference_worker(configs_dict, task_queue, result_queue):\n",
                "    \"\"\"\n",
                "    Persistent worker for VLA model inference.\n",
                "    \"\"\"\n",
                "    from vitra.models import load_model\n",
                "    from vitra.utils.data_utils import load_normalizer\n",
                "    from vitra.datasets.human_dataset import pad_state_human, pad_action\n",
                "    from vitra.datasets.dataset_utils import ActionFeature, StateFeature\n",
                "    \n",
                "    model = None\n",
                "    normalizer = None\n",
                "    \n",
                "    try:\n",
                "        print(\"[VLA Process] Loading VLA model...\")\n",
                "        model = load_model(configs_dict).cuda()\n",
                "        model.eval()\n",
                "        normalizer = load_normalizer(configs_dict)\n",
                "        print(f\"[VLA Process] VLA model ready.\")\n",
                "        \n",
                "        result_queue.put({'type': 'ready'})\n",
                "        \n",
                "        while True:\n",
                "            task = task_queue.get()\n",
                "            if task['type'] == 'shutdown':\n",
                "                break\n",
                "            elif task['type'] == 'predict':\n",
                "                try:\n",
                "                    image = task['image']\n",
                "                    instruction = task['instruction']\n",
                "                    state = task['state']\n",
                "                    state_mask = task['state_mask']\n",
                "                    action_mask = task['action_mask']\n",
                "                    fov = task['fov']\n",
                "                    num_ddim_steps = task.get('num_ddim_steps', 10)\n",
                "                    cfg_scale = task.get('cfg_scale', 5.0)\n",
                "                    sample_times = task.get('sample_times', 1)\n",
                "                    \n",
                "                    norm_state = normalizer.normalize_state(state.copy())\n",
                "                    unified_action_dim = ActionFeature.ALL_FEATURES[1]\n",
                "                    unified_state_dim = StateFeature.ALL_FEATURES[1]\n",
                "                    \n",
                "                    unified_state, unified_state_mask = pad_state_human(\n",
                "                        state=norm_state,\n",
                "                        state_mask=state_mask,\n",
                "                        action_dim=normalizer.action_mean.shape[0],\n",
                "                        state_dim=normalizer.state_mean.shape[0],\n",
                "                        unified_state_dim=unified_state_dim,\n",
                "                    )\n",
                "                    _, unified_action_mask = pad_action(\n",
                "                        actions=None,\n",
                "                        action_mask=action_mask.copy(),\n",
                "                        action_dim=normalizer.action_mean.shape[0],\n",
                "                        unified_action_dim=unified_action_dim\n",
                "                    )\n",
                "                    \n",
                "                    fov = torch.from_numpy(fov).unsqueeze(0)\n",
                "                    unified_state = unified_state.unsqueeze(0)\n",
                "                    unified_state_mask = unified_state_mask.unsqueeze(0)\n",
                "                    unified_action_mask = unified_action_mask.unsqueeze(0)\n",
                "                    \n",
                "                    norm_action = model.predict_action(\n",
                "                        image=image,\n",
                "                        instruction=instruction,\n",
                "                        current_state=unified_state,\n",
                "                        current_state_mask=unified_state_mask,\n",
                "                        action_mask_torch=unified_action_mask,\n",
                "                        num_ddim_steps=num_ddim_steps,\n",
                "                        cfg_scale=cfg_scale,\n",
                "                        fov=fov,\n",
                "                        sample_times=sample_times,\n",
                "                    )\n",
                "                    \n",
                "                    norm_action = norm_action[:, :, :102]\n",
                "                    unnorm_action = normalizer.unnormalize_action(norm_action)\n",
                "                    \n",
                "                    if isinstance(unnorm_action, torch.Tensor):\n",
                "                        unnorm_action_np = unnorm_action.cpu().numpy()\n",
                "                    else:\n",
                "                        unnorm_action_np = np.array(unnorm_action)\n",
                "                    \n",
                "                    result_queue.put({'type': 'result', 'success': True, 'data': unnorm_action_np})\n",
                "                except Exception as e:\n",
                "                    import traceback\n",
                "                    result_queue.put({'type': 'result', 'success': False, 'error': str(e), 'traceback': traceback.format_exc()})\n",
                "    except Exception as e:\n",
                "        import traceback\n",
                "        result_queue.put({'type': 'error', 'error': str(e), 'traceback': traceback.format_exc()})\n",
                "    finally:\n",
                "        if model is not None: del model\n",
                "        if normalizer is not None: del normalizer\n",
                "        torch.cuda.empty_cache()\n",
                "        torch.cuda.synchronize()\n",
                "        print(\"[VLA Process] Exiting\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Service Classes"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class HandReconstructionService:\n",
                "    def __init__(self, args_dict):\n",
                "        self.ctx = mp.get_context('spawn')\n",
                "        self.task_queue = self.ctx.Queue()\n",
                "        self.result_queue = self.ctx.Queue()\n",
                "        self.process = self.ctx.Process(target=_hand_reconstruction_worker, args=(args_dict, self.task_queue, self.result_queue))\n",
                "        self.process.start()\n",
                "        ready_msg = self.result_queue.get()\n",
                "        if ready_msg['type'] == 'ready':\n",
                "            print(\"Hand reconstruction service initialized\")\n",
                "        elif ready_msg['type'] == 'error':\n",
                "            raise RuntimeError(f\"Failed to initialize hand reconstruction: {ready_msg['error']}\")\n",
                "    \n",
                "    def reconstruct(self, image_path):\n",
                "        self.task_queue.put({'type': 'reconstruct', 'image_path': image_path})\n",
                "        result = self.result_queue.get()\n",
                "        if result['type'] == 'result' and result['success']:\n",
                "            return result['data']\n",
                "        else:\n",
                "            raise RuntimeError(f\"Hand reconstruction failed: {result.get('error', 'Unknown error')}\")\n",
                "    \n",
                "    def shutdown(self):\n",
                "        self.task_queue.put({'type': 'shutdown'})\n",
                "        self.process.join(timeout=10)\n",
                "        if self.process.is_alive():\n",
                "            self.process.terminate()\n",
                "            self.process.join()\n",
                "\n",
                "class VLAInferenceService:\n",
                "    def __init__(self, configs):\n",
                "        self.ctx = mp.get_context('spawn')\n",
                "        self.task_queue = self.ctx.Queue()\n",
                "        self.result_queue = self.ctx.Queue()\n",
                "        self.process = self.ctx.Process(target=_vla_inference_worker, args=(configs, self.task_queue, self.result_queue))\n",
                "        self.process.start()\n",
                "        ready_msg = self.result_queue.get()\n",
                "        if ready_msg['type'] == 'ready':\n",
                "            print(\"VLA inference service initialized\")\n",
                "        elif ready_msg['type'] == 'error':\n",
                "            raise RuntimeError(f\"Failed to initialize VLA model: {ready_msg['error']}\")\n",
                "    \n",
                "    def predict(self, image, instruction, state, state_mask, action_mask, fov, num_ddim_steps=10, cfg_scale=5.0, sample_times=1):\n",
                "        self.task_queue.put({\n",
                "            'type': 'predict', 'image': image, 'instruction': instruction, 'state': state,\n",
                "            'state_mask': state_mask, 'action_mask': action_mask, 'fov': fov,\n",
                "            'num_ddim_steps': num_ddim_steps, 'cfg_scale': cfg_scale, 'sample_times': sample_times,\n",
                "        })\n",
                "        result = self.result_queue.get()\n",
                "        if result['type'] == 'result' and result['success']:\n",
                "            return result['data']\n",
                "        else:\n",
                "            raise RuntimeError(f\"VLA inference failed: {result.get('error', 'Unknown error')}\")\n",
                "    \n",
                "    def shutdown(self):\n",
                "        self.task_queue.put({'type': 'shutdown'})\n",
                "        self.process.join(timeout=10)\n",
                "        if self.process.is_alive():\n",
                "            self.process.terminate()\n",
                "            self.process.join()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Configuration and Initialization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class Args:\n",
                "    def __init__(self):\n",
                "        # Model Configuration\n",
                "        self.config_path = '../config/human_vla.json'\n",
                "        self.model_path = None\n",
                "        self.statistics_path = None\n",
                "        \n",
                "        # Input/Output\n",
                "        self.image_path = '../data/examples/human_example.jpg'\n",
                "        self.hand_path = None\n",
                "        self.video_path = './example_human_inf.mp4'\n",
                "        \n",
                "        # Hand Reconstruction Models\n",
                "        self.hawor_model_path = '../weights/hawor/checkpoints/hawor.ckpt'\n",
                "        self.detector_path = '../weights/hawor/external/detector.pt'\n",
                "        self.moge_model_name = 'Ruicheng/moge-2-vitl'\n",
                "        self.mano_path = '../weights/mano'\n",
                "        \n",
                "        # Prediction Settings\n",
                "        self.use_left = True\n",
                "        self.use_right = True\n",
                "        self.instruction = \"Left: Put the trash into the garbage. Right: None.\"\n",
                "        self.sample_times = 4\n",
                "        self.fps = 8\n",
                "        self.save_state_local = True\n",
                "\n",
                "args = Args()\n",
                "configs = load_config(args.config_path)\n",
                "if args.model_path: configs['model_load_path'] = args.model_path\n",
                "if args.statistics_path: configs['statistics_path'] = args.statistics_path\n",
                "\n",
                "image_path_obj = Path(args.image_path)\n",
                "npy_path = image_path_obj.with_suffix('.npy')\n",
                "\n",
                "hand_data = None\n",
                "hand_recon_service = None\n",
                "\n",
                "if npy_path.exists():\n",
                "    print(f\"Found precomputed hand state results: {npy_path}.\")\n",
                "    hand_data = np.load(npy_path, allow_pickle=True).item()\n",
                "else:\n",
                "    recon_args_dict = {\n",
                "        'hawor_model_path': args.hawor_model_path,\n",
                "        'detector_path': args.detector_path,\n",
                "        'moge_model_name': args.moge_model_name,\n",
                "        'mano_path': args.mano_path,\n",
                "    }\n",
                "    hand_recon_service = HandReconstructionService(recon_args_dict)\n",
                "\n",
                "vla_service = VLAInferenceService(configs)\n",
                "\n",
                "hand_config = HandConfig(args)\n",
                "hand_config.FPS = args.fps\n",
                "visualizer = HandVisualizer(hand_config, render_gradual_traj=False)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Run Inference and Visualization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "try:\n",
                "    if hand_data is None:\n",
                "        print(\"Running hand reconstruction...\")\n",
                "        hand_data = hand_recon_service.reconstruct(args.image_path)\n",
                "        if args.save_state_local:\n",
                "            np.save(npy_path, hand_data, allow_pickle=True)\n",
                "            print(f\"Saved reconstructed hand state to {npy_path}\")\n",
                "\n",
                "    image = Image.open(args.image_path)\n",
                "    ori_w, ori_h = image.size\n",
                "    try:\n",
                "        image = ImageOps.exif_transpose(image)\n",
                "    except Exception: pass\n",
                "\n",
                "    image_resized = resize_short_side_to_target(image, target=224)\n",
                "    w, h = image_resized.size\n",
                "\n",
                "    current_state_left, current_state_right = None, None\n",
                "    if args.use_right:\n",
                "        current_state_right, beta_right, fov_x, _ = get_state(hand_data, hand_side='right')\n",
                "    if args.use_left:\n",
                "        current_state_left, beta_left, fov_x, _ = get_state(hand_data, hand_side='left')\n",
                "    \n",
                "    fov_x_rad = fov_x * np.pi / 180\n",
                "    f_ori = ori_w / np.tan(fov_x_rad / 2) / 2\n",
                "    fov_y_rad = 2 * np.arctan(ori_h / (2 * f_ori))\n",
                "    f = w / np.tan(fov_x_rad / 2) / 2\n",
                "    intrinsics = np.array([[f, 0, w/2], [0, f, h/2], [0, 0, 1]])\n",
                "\n",
                "    state_left = current_state_left if args.use_left else np.zeros_like(current_state_right)\n",
                "    beta_left = beta_left if args.use_left else np.zeros_like(beta_right)\n",
                "    state_right = current_state_right if args.use_right else np.zeros_like(current_state_left)\n",
                "    beta_right = beta_right if args.use_right else np.zeros_like(beta_left)\n",
                "    \n",
                "    state = np.concatenate([state_left, beta_left, state_right, beta_right], axis=0)\n",
                "    state_mask = np.array([args.use_left, args.use_right], dtype=bool)\n",
                "    chunk_size = configs.get('fwd_pred_next_n', 16)\n",
                "    action_mask = np.tile(np.array([[args.use_left, args.use_right]], dtype=bool), (chunk_size, 1)) \n",
                "    fov = np.array([fov_x_rad, fov_y_rad], dtype=np.float32)\n",
                "    image_resized_np = np.array(image_resized)\n",
                "\n",
                "    print(f\"Running VLA inference...\")\n",
                "    unnorm_action = vla_service.predict(\n",
                "        image=image_resized_np, instruction=args.instruction, state=state,\n",
                "        state_mask=state_mask, action_mask=action_mask, fov=fov,\n",
                "        num_ddim_steps=10, cfg_scale=5.0, sample_times=args.sample_times,\n",
                "    )\n",
                "    \n",
                "    fx_exo, fy_exo = intrinsics[0, 0], intrinsics[1, 1]\n",
                "    renderer = Renderer(w, h, (fx_exo, fy_exo), 'cuda')\n",
                "    T = len(action_mask) + 1\n",
                "    traj_mask = np.tile(np.array([[args.use_left, args.use_right]], dtype=bool), (T, 1)) \n",
                "    hand_mask = (traj_mask[:, 0], traj_mask[:, 1])\n",
                "    all_rendered_frames = []\n",
                "    \n",
                "    for i in range(args.sample_times):\n",
                "        traj_left = recon_traj(state=state_left, rel_action=unnorm_action[i, :, 0:51]) if args.use_left else np.zeros((T, 51))\n",
                "        traj_right = recon_traj(state=state_right, rel_action=unnorm_action[i, :, 51:102]) if args.use_right else np.zeros((T, 51))\n",
                "        \n",
                "        left_hand_labels = {\n",
                "            'transl_worldspace': traj_left[:, 0:3],\n",
                "            'global_orient_worldspace': R.from_euler('xyz', traj_left[:, 3:6]).as_matrix(),\n",
                "            'hand_pose': euler_traj_to_rotmat_traj(traj_left[:, 6:51], T),\n",
                "            'beta': beta_left,\n",
                "        }\n",
                "        right_hand_labels = {\n",
                "            'transl_worldspace': traj_right[:, 0:3],\n",
                "            'global_orient_worldspace': R.from_euler('xyz', traj_right[:, 3:6]).as_matrix(),\n",
                "            'hand_pose': euler_traj_to_rotmat_traj(traj_right[:, 6:51], T),\n",
                "            'beta': beta_right,\n",
                "        }\n",
                "        verts_left, _ = process_single_hand_labels(left_hand_labels, hand_mask[0], visualizer.mano, is_left=True)\n",
                "        verts_right, _ = process_single_hand_labels(right_hand_labels, hand_mask[1], visualizer.mano, is_left=False)\n",
                "        \n",
                "        extrinsics = (np.broadcast_to(np.eye(3), (T, 3, 3)).copy(), np.zeros((T, 3, 1), dtype=np.float32))\n",
                "        save_frames = visualizer._render_hand_trajectory([image_resized_np[..., ::-1]] * T, (verts_left, verts_right), hand_mask, extrinsics, renderer, mode='first')\n",
                "        all_rendered_frames.append(save_frames)\n",
                "    \n",
                "    grid_cols = math.ceil(math.sqrt(args.sample_times))\n",
                "    grid_rows = math.ceil(args.sample_times / grid_cols)\n",
                "    combined_frames = []\n",
                "    for frame_idx in range(T):\n",
                "        sample_frames = [all_rendered_frames[i][frame_idx] for i in range(args.sample_times)]\n",
                "        while len(sample_frames) < grid_rows * grid_cols:\n",
                "            sample_frames.append(np.zeros_like(sample_frames[0]))\n",
                "        rows = [np.concatenate(sample_frames[r*grid_cols:(r+1)*grid_cols], axis=1) for r in range(grid_rows)]\n",
                "        combined_frames.append(np.concatenate(rows, axis=0))\n",
                "\n",
                "    save_to_video(combined_frames, f'{args.video_path}', fps=hand_config.FPS)\n",
                "    print(f\"Combined video saved to {args.video_path}\")\n",
                "\n",
                "finally:\n",
                "    print(\"Shutting down services...\")\n",
                "    if hand_recon_service is not None: hand_recon_service.shutdown()\n",
                "    vla_service.shutdown()\n",
                "    print(\"All services shut down\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Display Results\n",
                "\n",
                "You can use the following cell to display the generated video in the notebook."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from IPython.display import Video\n",
                "Video(args.video_path, embed=True)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}