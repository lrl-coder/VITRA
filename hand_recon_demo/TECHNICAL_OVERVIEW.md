# 3Dæ‰‹éƒ¨é‡å»º - æŠ€æœ¯æ¦‚è¿°ä¸å®ç°æµç¨‹

æœ¬æ–‡æ¡£è¯¦ç»†ä»‹ç»äº† `hand_recon_demo` é¡¹ç›®çš„æŠ€æœ¯æ€è·¯ã€å®ç°æµç¨‹å’Œæ ¸å¿ƒåŸç†ã€‚

## ğŸ“š ç›®å½•

- [1. é¡¹ç›®æ¦‚è¿°](#1-é¡¹ç›®æ¦‚è¿°)
- [2. æŠ€æœ¯æ¶æ„](#2-æŠ€æœ¯æ¶æ„)
- [3. æ ¸å¿ƒæ¨¡å—è¯¦è§£](#3-æ ¸å¿ƒæ¨¡å—è¯¦è§£)
- [4. æ•°æ®æµä¸å¤„ç†æµç¨‹](#4-æ•°æ®æµä¸å¤„ç†æµç¨‹)
- [5. åæ ‡ç³»ç»Ÿä¸å˜æ¢](#5-åæ ‡ç³»ç»Ÿä¸å˜æ¢)
- [6. å…³é”®æŠ€æœ¯ç»†èŠ‚](#6-å…³é”®æŠ€æœ¯ç»†èŠ‚)
- [7. ä¿å­˜çš„ä½å§¿æ•°æ®ç»“æ„](#7-ä¿å­˜çš„ä½å§¿æ•°æ®ç»“æ„)
- [8. å¸¸è§é—®é¢˜ä¸è°ƒè¯•](#8-å¸¸è§é—®é¢˜ä¸è°ƒè¯•)
- [9. æ‰©å±•ä¸æ”¹è¿›æ–¹å‘](#9-æ‰©å±•ä¸æ”¹è¿›æ–¹å‘)

---

## 1. é¡¹ç›®æ¦‚è¿°

### 1.1 ç›®æ ‡

ä»è§†é¢‘æˆ–å›¾åƒåºåˆ—ä¸­é‡å»º3Dæ‰‹éƒ¨æ¨¡å‹ï¼Œå¹¶ç”Ÿæˆé«˜è´¨é‡çš„å¯è§†åŒ–è§†é¢‘ã€‚

### 1.2 æ ¸å¿ƒç‰¹æ€§

- **å·²çŸ¥ç›¸æœºå†…å‚**ï¼šæ— éœ€é€šè¿‡MoGeä¼°è®¡ç›¸æœºå‚æ•°ï¼Œç›´æ¥ä½¿ç”¨ç”¨æˆ·æä¾›çš„ç›¸æœºå†…å‚çŸ©é˜µ
- **é«˜æ•ˆæµæ°´çº¿**ï¼šç®€åŒ–äº†å¤„ç†æµç¨‹ï¼Œæé«˜äº†å¤„ç†é€Ÿåº¦
- **é«˜è´¨é‡æ¸²æŸ“**ï¼šåŸºäºPyTorch3Dçš„ä¸“ä¸šçº§3Dæ¸²æŸ“
- **æ•°æ®å¯¼å‡º**ï¼šæ”¯æŒä¿å­˜æ‰‹éƒ¨ä½å§¿æ•°æ®ä¾›åç»­ä½¿ç”¨

### 1.3 æŠ€æœ¯æ ˆ

| ç»„ä»¶ | æŠ€æœ¯ | ç”¨é€” |
|------|------|------|
| æ‰‹éƒ¨æ£€æµ‹ | YOLOv8 | å®šä½å›¾åƒä¸­çš„æ‰‹éƒ¨åŒºåŸŸ |
| å§¿æ€ä¼°è®¡ | HaWoR | ä¼°è®¡æ‰‹éƒ¨å§¿æ€ã€å½¢çŠ¶å’Œä½ç§» |
| 3Då»ºæ¨¡ | MANO | å‚æ•°åŒ–æ‰‹éƒ¨æ¨¡å‹ |
| æ¸²æŸ“ | PyTorch3D | é«˜è´¨é‡3Dæ¸²æŸ“ |
| æ¡†æ¶ | PyTorch | æ·±åº¦å­¦ä¹ æ¨ç† |

---

## 2. æŠ€æœ¯æ¶æ„

### 2.1 æ•´ä½“æ¶æ„å›¾

```
è¾“å…¥å›¾åƒ/è§†é¢‘
    â†“
[1. æ‰‹éƒ¨æ£€æµ‹] (detector.pt)
    â†“
æ£€æµ‹æ¡† + ç½®ä¿¡åº¦
    â†“
[2. å§¿æ€ä¼°è®¡] (HaWoR)
    â†“
æ‰‹éƒ¨å‚æ•° (Î², Î¸, R, t)
    â†“
[3. MANOå»ºæ¨¡]
    â†“
3Dé¡¶ç‚¹ + å…³èŠ‚ç‚¹
    â†“
[4. åæ ‡å¯¹é½]
    â†“
ç›¸æœºåæ ‡ç³»ä¸‹çš„é¡¶ç‚¹
    â†“
[5. PyTorch3Dæ¸²æŸ“]
    â†“
è¾“å‡ºè§†é¢‘
```

### 2.2 æ¨¡å—ç»„æˆ

```
hand_recon_demo/
â”œâ”€â”€ demo.py                      # ä¸»ç¨‹åºï¼šæµç¨‹æ§åˆ¶
â”œâ”€â”€ hand_recon_known_camera.py   # é‡å»ºæ¨¡å—ï¼šæ‰‹éƒ¨å‚æ•°ä¼°è®¡
â”œâ”€â”€ visualizer.py                # å¯è§†åŒ–æ¨¡å—ï¼š3Dæ¸²æŸ“
â””â”€â”€ load_hand_pose.py            # å·¥å…·ï¼šåŠ è½½å’Œå¯è§†åŒ–ä¿å­˜çš„ä½å§¿
```

---

## 3. æ ¸å¿ƒæ¨¡å—è¯¦è§£

### 3.1 demo.py - ä¸»ç¨‹åº

**èŒè´£**ï¼šåè°ƒæ•´ä¸ªå¤„ç†æµç¨‹

**æ ¸å¿ƒåŠŸèƒ½**ï¼š
1. è§£æå‘½ä»¤è¡Œå‚æ•°
2. åŠ è½½è¾“å…¥æ•°æ®ï¼ˆè§†é¢‘/å›¾åƒåºåˆ—ï¼‰
3. åˆ›å»ºç›¸æœºå†…å‚çŸ©é˜µ
4. è°ƒç”¨é‡å»ºæ¨¡å—
5. è°ƒç”¨å¯è§†åŒ–æ¨¡å—
6. ä¿å­˜ç»“æœ

**å…³é”®ä»£ç **ï¼š
```python
# åˆ›å»ºç›¸æœºå†…å‚çŸ©é˜µ
camera_intrinsics = create_camera_intrinsics(
    fx=args.camera_fx,
    fy=args.camera_fy,
    cx=args.camera_cx,
    cy=args.camera_cy,
    image_width=W,
    image_height=H
)

# æ‰§è¡Œé‡å»º
recon_results = reconstructor.recon(
    images=images,
    camera_intrinsics=camera_intrinsics,
    thresh=args.thresh
)

# ç”Ÿæˆå¯è§†åŒ–
visualizer.create_video_with_3d_hands(
    images=images,
    recon_results=recon_results,
    camera_intrinsics=camera_intrinsics,
    output_path=args.output,
    fps=output_fps
)
```

---

### 3.2 hand_recon_known_camera.py - é‡å»ºæ¨¡å—

**èŒè´£**ï¼šæ‰§è¡Œ3Dæ‰‹éƒ¨é‡å»ºçš„æ ¸å¿ƒç®—æ³•

#### 3.2.1 åˆå§‹åŒ–

```python
class HandReconstructorWithKnownCamera:
    def __init__(self, hawor_model_path, detector_path, mano_path, device):
        # 1. åˆå§‹åŒ– HaWoR æµæ°´çº¿
        self.hawor_pipeline = HaworPipeline(
            model_path=hawor_model_path,
            detector_path=detector_path,
            device=device
        )
        
        # 2. åˆå§‹åŒ– MANO æ¨¡å‹
        self.mano = MANO(model_path=mano_path).to(device)
```

#### 3.2.2 é‡å»ºæµç¨‹

```python
def recon(self, images, camera_intrinsics, thresh):
    # Step 1: æå–ç›¸æœºç„¦è·
    fx = camera_intrinsics[0, 0]
    fy = camera_intrinsics[1, 1]
    img_focal = (fx + fy) / 2.0
    
    # Step 2: HaWoR å§¿æ€ä¼°è®¡
    recon_results = self.hawor_pipeline.recon(
        images, img_focal, thresh=thresh
    )
    
    # Step 3: åæ ‡å¯¹é½
    for img_idx, hand_type in enumerate(['left', 'right']):
        # 3.1 MANO å‰å‘ä¼ æ’­
        model_output = self.mano(
            betas=betas,
            hand_pose=hand_pose
        )
        
        # 3.2 å·¦æ‰‹é•œåƒç¿»è½¬
        if hand_type == 'left':
            verts[:, 0] = -verts[:, 0]
            joints[:, 0] = -joints[:, 0]
        
        # 3.3 è®¡ç®—ä¿®æ­£åçš„å…¨å±€ä½ç§»
        wrist = joints[0]
        transl_aligned = wrist + transl
    
    return recon_results_aligned
```

**å…³é”®ç‚¹**ï¼š
- HaWoRè¾“å‡ºçš„æ˜¯**ç›¸å¯¹äºæ‰‹è…•çš„å±€éƒ¨åæ ‡**
- éœ€è¦é€šè¿‡ `transl_aligned = wrist + transl` ä¿®æ­£åˆ°**ç›¸æœºåæ ‡ç³»**

---

### 3.3 visualizer.py - å¯è§†åŒ–æ¨¡å—

**èŒè´£**ï¼šä½¿ç”¨PyTorch3Dæ¸²æŸ“3Dæ‰‹éƒ¨æ¨¡å‹

#### 3.3.1 æ¶æ„è®¾è®¡

```python
class HandVisualizer(BaseHandVisualizer):
    """ç»§æ‰¿è‡ª VITRA çš„ BaseHandVisualizer"""
    
    def __init__(self, mano_path):
        config = DemoConfig(mano_path=mano_path)
        super().__init__(config, render_gradual_traj=False)
        self.all_modes = ['cam']  # ä»…ä½¿ç”¨ç›¸æœºæ¨¡å¼
```

#### 3.3.2 æ¸²æŸ“æµç¨‹

```python
def create_video_with_3d_hands(self, images, recon_results, camera_intrinsics, output_path, fps):
    # Step 1: å‡†å¤‡æ•°æ®å®¹å™¨
    verts_left_list = np.zeros((T, 778, 3))   # å·¦æ‰‹é¡¶ç‚¹
    verts_right_list = np.zeros((T, 778, 3))  # å³æ‰‹é¡¶ç‚¹
    mask_left = np.zeros(T)                   # å·¦æ‰‹æ©ç 
    mask_right = np.zeros(T)                  # å³æ‰‹æ©ç 
    
    # Step 2: å¡«å……æ‰‹éƒ¨æ•°æ®
    for t in range(T):
        # 2.1 MANO å‰å‘ä¼ æ’­ï¼ˆç”Ÿæˆé¡¶ç‚¹ï¼‰
        mano_out = mano(betas=beta, hand_pose=hand_pose, global_orient=identity_rot)
        verts = mano_out.vertices[0]
        joints = mano_out.joints[0]
        
        # 2.2 å·¦æ‰‹Xè½´ç¿»è½¬
        if hand_type == 'left':
            verts[:, 0] *= -1
            joints[:, 0] *= -1
        
        # 2.3 åº”ç”¨å…¨å±€æ—‹è½¬å’Œå¹³ç§»
        wrist = joints[0]
        verts_cam = (global_orient @ (verts - wrist).T).T + transl
    
    # Step 3: å‡†å¤‡ç›¸æœºå‚æ•°
    R_w2c = np.eye(3)  # ä¸–ç•Œåæ ‡ç³» = ç›¸æœºåæ ‡ç³»
    t_w2c = np.zeros((3, 1))
    
    # Step 4: åˆå§‹åŒ–æ¸²æŸ“å™¨
    renderer = Renderer(W, H, (fx, fy), device)
    
    # Step 5: æ‰§è¡Œæ¸²æŸ“
    rendered_frames = self._render_hand_trajectory(
        video_frames=images,
        hand_traj_wordspace=(verts_left_list, verts_right_list),
        hand_mask=(mask_left, mask_right),
        extrinsics=(R_w2c, t_w2c),
        renderer=renderer,
        mode='cam'
    )
    
    # Step 6: ä¿å­˜è§†é¢‘
    out = cv2.VideoWriter(output_path, fourcc, fps, (W, H))
    for frame in rendered_frames:
        frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)
        out.write(frame_bgr)
```

---

## 4. æ•°æ®æµä¸å¤„ç†æµç¨‹

### 4.1 æ•°æ®æµå›¾

```
åŸå§‹å›¾åƒ (H, W, 3) BGR
    â†“
[æ‰‹éƒ¨æ£€æµ‹å™¨]
    â†“
æ£€æµ‹æ¡† {left/right: [x1, y1, x2, y2], conf}
    â†“
[HaWoR å§¿æ€ä¼°è®¡]
    â†“
æ‰‹éƒ¨å‚æ•° {
    beta: (10,)           # å½¢çŠ¶å‚æ•°
    hand_pose: (15, 3, 3) # æ‰‹æŒ‡å…³èŠ‚æ—‹è½¬
    global_orient: (3, 3) # å…¨å±€æ—‹è½¬
    transl: (3,)          # å…¨å±€ä½ç§»ï¼ˆæœªå¯¹é½ï¼‰
}
    â†“
[åæ ‡å¯¹é½]
    â†“
å¯¹é½å‚æ•° {
    transl_aligned: (3,)  # ç›¸æœºåæ ‡ç³»ä¸‹çš„æ‰‹è…•ä½ç½®
    ... (å…¶ä»–å‚æ•°ä¸å˜)
}
    â†“
[MANO å»ºæ¨¡]
    â†“
æ‰‹éƒ¨ç½‘æ ¼ {
    vertices: (778, 3)    # é¡¶ç‚¹åæ ‡ï¼ˆå±€éƒ¨ï¼‰
    joints: (21, 3)       # å…³èŠ‚åæ ‡ï¼ˆå±€éƒ¨ï¼‰
}
    â†“
[å…¨å±€å˜æ¢]
    â†“
ç›¸æœºåæ ‡ç³»é¡¶ç‚¹ = global_orient @ (vertices - wrist) + transl_aligned
    â†“
[PyTorch3D æ¸²æŸ“]
    â†“
æ¸²æŸ“å›¾åƒ (H, W, 3) RGB
```

### 4.2 å¤„ç†æ­¥éª¤è¯¦è§£

#### Step 1: å›¾åƒåŠ è½½

**è¾“å…¥**ï¼šè§†é¢‘æ–‡ä»¶æˆ–å›¾åƒæ–‡ä»¶å¤¹  
**è¾“å‡º**ï¼šå›¾åƒåˆ—è¡¨ `List[np.ndarray]`ï¼Œæ¯ä¸ªå›¾åƒä¸º `(H, W, 3)` BGRæ ¼å¼

```python
# è§†é¢‘åŠ è½½
cap = cv2.VideoCapture(video_path)
while True:
    ret, frame = cap.read()
    if not ret: break
    images.append(frame)  # BGRæ ¼å¼

# å›¾åƒåºåˆ—åŠ è½½
image_files = sorted(Path(folder_path).glob('*.jpg'))
images = [cv2.imread(str(f)) for f in image_files]
```

#### Step 2: æ‰‹éƒ¨æ£€æµ‹

**æ¨¡å‹**ï¼šYOLOv8 (`detector.pt`)  
**è¾“å…¥**ï¼šBGRå›¾åƒ  
**è¾“å‡º**ï¼šæ£€æµ‹æ¡† `[x1, y1, x2, y2]` + ç½®ä¿¡åº¦

```python
# HaWoRå†…éƒ¨è°ƒç”¨æ£€æµ‹å™¨
detections = detector(image)
for det in detections:
    if det.conf > thresh:
        bbox = det.bbox  # [x1, y1, x2, y2]
        hand_type = det.label  # 'left' or 'right'
```

#### Step 3: å§¿æ€ä¼°è®¡ï¼ˆHaWoRï¼‰

**æ¨¡å‹**ï¼šHaWoR (`hawor.ckpt`)  
**è¾“å…¥**ï¼šè£å‰ªåçš„æ‰‹éƒ¨å›¾åƒ  
**è¾“å‡º**ï¼šMANOå‚æ•°

```python
# HaWoR æ¨ç†
hand_crop = crop_bbox(image, bbox)
params = hawor_model(hand_crop)

# è¾“å‡º
beta = params['beta']              # (10,) å½¢çŠ¶å‚æ•°
hand_pose = params['hand_pose']    # (15, 3, 3) å…³èŠ‚æ—‹è½¬çŸ©é˜µ
global_orient = params['global_orient']  # (3, 3) å…¨å±€æ—‹è½¬
transl = params['transl']          # (3,) å…¨å±€ä½ç§»ï¼ˆç›¸å¯¹ï¼‰
```

**å…³é”®**ï¼šHaWoRçš„ `transl` æ˜¯**ç›¸å¯¹äºMANOæ¨¡å‹åŸç‚¹**çš„åç§»ï¼Œéœ€è¦è¿›ä¸€æ­¥å¯¹é½ã€‚

#### Step 4: åæ ‡å¯¹é½

**é—®é¢˜**ï¼šHaWoRè¾“å‡ºçš„ `transl` ä¸æ˜¯æ‰‹è…•çš„çœŸå®3Dä½ç½®

**è§£å†³**ï¼šé€šè¿‡MANOå‰å‘ä¼ æ’­è·å–æ‰‹è…•åæ ‡ï¼Œè¡¥å¿åç§»

```python
# MANOå‰å‘ä¼ æ’­ï¼ˆæ— å…¨å±€æ—‹è½¬ï¼‰
mano_out = mano(betas=beta, hand_pose=hand_pose, global_orient=I)
verts = mano_out.vertices[0]  # (778, 3)
joints = mano_out.joints[0]   # (21, 3)

# å·¦æ‰‹é•œåƒ
if hand_type == 'left':
    verts[:, 0] = -verts[:, 0]
    joints[:, 0] = -joints[:, 0]

# è·å–æ‰‹è…•ä½ç½®ï¼ˆå…³èŠ‚0ï¼‰
wrist = joints[0]  # (3,)

# ä¿®æ­£å…¨å±€ä½ç§»
transl_aligned = wrist + transl  # ç›¸æœºåæ ‡ç³»ä¸‹çš„æ‰‹è…•çœŸå®ä½ç½®
```

**æ•°å­¦åŸç†**ï¼š
```
MANOå±€éƒ¨åæ ‡: V_local, J_local
HaWoRé¢„æµ‹ä½ç§»: t_pred (ç›¸å¯¹äºæ¨¡å‹åŸç‚¹)
æ‰‹è…•å±€éƒ¨åæ ‡: J_0 (MANOè¾“å‡º)

ç›¸æœºåæ ‡ç³»æ‰‹è…•ä½ç½®:
    t_aligned = J_0 + t_pred
```

#### Step 5: MANOå»ºæ¨¡

**æ¨¡å‹**ï¼šMANOå‚æ•°åŒ–æ‰‹éƒ¨æ¨¡å‹  
**è¾“å…¥**ï¼šå½¢çŠ¶å‚æ•°Î²ã€å§¿æ€å‚æ•°Î¸  
**è¾“å‡º**ï¼š3Dé¡¶ç‚¹å’Œå…³èŠ‚

```python
# MANOå‰å‘ä¼ æ’­
output = MANO(
    betas=beta,        # (1, 10) å½¢çŠ¶å‚æ•°
    hand_pose=theta    # (1, 15, 3, 3) å§¿æ€å‚æ•°
)

vertices = output.vertices  # (1, 778, 3) æ‰‹éƒ¨é¡¶ç‚¹
joints = output.joints      # (1, 21, 3) æ‰‹éƒ¨å…³èŠ‚
```

**é¡¶ç‚¹æ•°é‡**ï¼š778ä¸ªé¡¶ç‚¹å®šä¹‰æ‰‹éƒ¨è¡¨é¢

#### Step 6: å…¨å±€å˜æ¢

**ç›®æ ‡**ï¼šå°†MANOå±€éƒ¨åæ ‡è½¬æ¢åˆ°ç›¸æœºåæ ‡ç³»

```python
# å…¨å±€å˜æ¢å…¬å¼
V_cam = R @ (V_local - J_wrist) + t_aligned

# ä»£ç å®ç°
verts_cam = (global_orient @ (verts - wrist).T).T + transl_aligned
```

**å˜æ¢æ­¥éª¤**ï¼š
1. å°†é¡¶ç‚¹å¹³ç§»åˆ°æ‰‹è…•åŸç‚¹ï¼š`V_local - J_wrist`
2. åº”ç”¨å…¨å±€æ—‹è½¬ï¼š`R @ ...`
3. å¹³ç§»åˆ°ç›¸æœºåæ ‡ç³»ï¼š`+ t_aligned`

#### Step 7: PyTorch3Dæ¸²æŸ“

**æ¡†æ¶**ï¼šPyTorch3D  
**è¾“å…¥**ï¼šç›¸æœºåæ ‡ç³»ä¸‹çš„é¡¶ç‚¹  
**è¾“å‡º**ï¼šæ¸²æŸ“å›¾åƒ

```python
# åˆå§‹åŒ–æ¸²æŸ“å™¨
renderer = Renderer(W, H, (fx, fy), device)

# æ¸²æŸ“
rendered_image = renderer.render(
    vertices=verts_cam,
    faces=mano.faces,
    colors=hand_color
)
```

---

## 5. åæ ‡ç³»ç»Ÿä¸å˜æ¢

### 5.1 æ¶‰åŠçš„åæ ‡ç³»

| åæ ‡ç³» | å®šä¹‰ | ç”¨é€” |
|--------|------|------|
| **å›¾åƒåæ ‡ç³»** | åŸç‚¹åœ¨å›¾åƒå·¦ä¸Šè§’ï¼Œxå‘å³ï¼Œyå‘ä¸‹ | 2Dæ£€æµ‹ã€å¯è§†åŒ– |
| **ç›¸æœºåæ ‡ç³»** | åŸç‚¹åœ¨ç›¸æœºå…‰å¿ƒï¼Œzè½´å‚ç›´äºå›¾åƒå¹³é¢ | 3Dé‡å»ºã€æŠ•å½± |
| **MANOå±€éƒ¨åæ ‡ç³»** | åŸç‚¹åœ¨æ¨¡å‹ä¸­å¿ƒï¼Œä¸æ‰‹éƒ¨æ— å…³ | MANOæ¨¡å‹è¾“å‡º |
| **ä¸–ç•Œåæ ‡ç³»** | æœ¬é¡¹ç›®ä¸­ç­‰åŒäºç›¸æœºåæ ‡ç³» | ç»Ÿä¸€è¡¨ç¤º |

### 5.2 åæ ‡å˜æ¢å…³ç³»

```
MANOå±€éƒ¨åæ ‡ --[å·¦æ‰‹é•œåƒ]--> MANOé•œåƒåæ ‡
                                |
                                v
                        [å¹³ç§»åˆ°æ‰‹è…•åŸç‚¹]
                                |
                                v
                          æ‰‹è…•å±€éƒ¨åæ ‡
                                |
                                v
                          [å…¨å±€æ—‹è½¬R]
                                |
                                v
                        [å…¨å±€å¹³ç§»t_aligned]
                                |
                                v
                          ç›¸æœºåæ ‡ç³»
                                |
                                v
                          [ç›¸æœºæŠ•å½±K]
                                |
                                v
                           å›¾åƒåæ ‡
```

### 5.3 å…³é”®å˜æ¢å…¬å¼

#### 3Dåˆ°2DæŠ•å½±

```python
# ç›¸æœºæŠ•å½±å…¬å¼
p_2d_homo = K @ p_3d  # (3,) = (3, 3) @ (3,)

# å½’ä¸€åŒ–
u = p_2d_homo[0] / p_2d_homo[2]
v = p_2d_homo[1] / p_2d_homo[2]

p_2d = [u, v]
```

å…¶ä¸­ç›¸æœºå†…å‚çŸ©é˜µï¼š
```
K = [[fx,  0, cx],
     [ 0, fy, cy],
     [ 0,  0,  1]]
```

#### å·¦æ‰‹é•œåƒç¿»è½¬

```python
# MANOæ¨¡å‹é»˜è®¤æ˜¯å³æ‰‹åæ ‡ç³»
# å·¦æ‰‹éœ€è¦æ²¿Xè½´ç¿»è½¬
if hand_type == 'left':
    vertices[:, 0] = -vertices[:, 0]
    joints[:, 0] = -joints[:, 0]
```

**åŸå› **ï¼šMANOæ¨¡å‹è®­ç»ƒæ—¶ä½¿ç”¨å³æ‰‹æ•°æ®ï¼Œå·¦æ‰‹é€šè¿‡é•œåƒå¾—åˆ°ã€‚

---

## 6. å…³é”®æŠ€æœ¯ç»†èŠ‚

### 6.1 ç›¸æœºå†…å‚çŸ©é˜µ

#### å®šä¹‰
```
K = [[fx,  0, cx],
     [ 0, fy, cy],
     [ 0,  0,  1]]
```

- `fx`, `fy`: ç„¦è·ï¼ˆåƒç´ å•ä½ï¼‰
- `cx`, `cy`: ä¸»ç‚¹åæ ‡ï¼ˆé€šå¸¸æ˜¯å›¾åƒä¸­å¿ƒï¼‰

#### è·å–æ–¹å¼

1. **ç›¸æœºæ ‡å®š**ï¼ˆæœ€å‡†ç¡®ï¼‰
   ```python
   # ä½¿ç”¨ OpenCV æ ‡å®š
   ret, K, dist, rvecs, tvecs = cv2.calibrateCamera(...)
   ```

2. **å·²çŸ¥FOV**
   ```python
   fx = image_width / (2 * tan(fov_x / 2))
   fy = image_height / (2 * tan(fov_y / 2))
   ```

3. **ç»éªŒä¼°ç®—**
   ```python
   # æ‰‹æœºç›¸æœºï¼šfx â‰ˆ fy â‰ˆ image_width
   fx = fy = W
   cx, cy = W/2, H/2
   ```

### 6.2 æ‰‹éƒ¨æ£€æµ‹ç½®ä¿¡åº¦é˜ˆå€¼

**å‚æ•°**ï¼š`thresh`ï¼ˆé»˜è®¤0.5ï¼‰

**ä½œç”¨**ï¼šè¿‡æ»¤ä½ç½®ä¿¡åº¦çš„æ£€æµ‹ç»“æœ

**è°ƒä¼˜å»ºè®®**ï¼š
- æ‰‹éƒ¨æ¸…æ™°ã€èƒŒæ™¯ç®€å•ï¼š`thresh = 0.6~0.7`
- æ‰‹éƒ¨æ¨¡ç³Šã€é®æŒ¡è¾ƒå¤šï¼š`thresh = 0.3~0.4`

```python
if detection.confidence > thresh:
    # ä¿ç•™è¯¥æ£€æµ‹
    process_hand(detection)
```

### 6.3 MANOæ¨¡å‹å‚æ•°

#### å½¢çŠ¶å‚æ•° Î²

- **ç»´åº¦**ï¼š`(10,)`
- **å«ä¹‰**ï¼šPCAé™ç»´åçš„æ‰‹éƒ¨å½¢çŠ¶ç³»æ•°
- **ä½œç”¨**ï¼šæ§åˆ¶æ‰‹éƒ¨å¤§å°ã€ç²—ç»†ç­‰ä¸ªä½“å·®å¼‚

```python
# Î² = 0: å¹³å‡æ‰‹éƒ¨å½¢çŠ¶
# Î² â‰  0: åç¦»å¹³å‡å½¢çŠ¶
beta = np.zeros(10)  # é»˜è®¤å¹³å‡å½¢çŠ¶
```

#### å§¿æ€å‚æ•° Î¸

- **ç»´åº¦**ï¼š`(15, 3, 3)` æˆ– `(45,)` (è½´è§’è¡¨ç¤º)
- **å«ä¹‰**ï¼š15ä¸ªæ‰‹æŒ‡å…³èŠ‚çš„æ—‹è½¬çŸ©é˜µ
- **ä½œç”¨**ï¼šæ§åˆ¶æ‰‹æŒ‡çš„å¼¯æ›²å’Œå±•å¼€

```python
# 15ä¸ªå…³èŠ‚å¯¹åº”ï¼š
# æ‹‡æŒ‡: 4ä¸ªå…³èŠ‚ (CMC, MCP, IP, TIP)
# é£ŸæŒ‡~å°æŒ‡: å„3ä¸ªå…³èŠ‚ (MCP, PIP, DIP) Ã— 4 = 12
# æ‰‹è…•: 1ä¸ªå…³èŠ‚
```

#### å…¨å±€æ—‹è½¬ R

- **ç»´åº¦**ï¼š`(3, 3)`
- **å«ä¹‰**ï¼šæ‰‹éƒ¨æ•´ä½“çš„æ—‹è½¬çŸ©é˜µ
- **ä½œç”¨**ï¼šæ§åˆ¶æ‰‹éƒ¨æœå‘

```python
# æ—‹è½¬çŸ©é˜µæ€§è´¨
# R @ R.T = I
# det(R) = 1
```

### 6.4 å·¦å³æ‰‹å¤„ç†

#### æ£€æµ‹æ ‡ç­¾

```python
hand_labels = {
    0: 'left',   # å·¦æ‰‹
    1: 'right'   # å³æ‰‹
}
```

#### é•œåƒç¿»è½¬

```python
# MANOé»˜è®¤å³æ‰‹ï¼Œå·¦æ‰‹éœ€è¦é•œåƒ
if hand_type == 'left':
    # æ²¿Xè½´ç¿»è½¬
    vertices[:, 0] *= -1
    joints[:, 0] *= -1
```

**æ³¨æ„**ï¼šç¿»è½¬åœ¨MANOå±€éƒ¨åæ ‡ç³»ä¸­è¿›è¡Œï¼Œä¹‹åå†åº”ç”¨å…¨å±€å˜æ¢ã€‚

### 6.5 PyTorch3Dæ¸²æŸ“é…ç½®

#### ç›¸æœºè®¾ç½®

```python
# ä½¿ç”¨é€è§†æŠ•å½±ç›¸æœº
cameras = PerspectiveCameras(
    focal_length=((fx, fy),),
    principal_point=((cx, cy),),
    device=device
)
```

#### å…‰ç…§è®¾ç½®

```python
# ç¯å¢ƒå…‰ + æ¼«åå°„å…‰
lights = AmbientLights(device=device)
```

#### æ¸²æŸ“å™¨

```python
renderer = MeshRenderer(
    rasterizer=MeshRasterizer(
        cameras=cameras,
        raster_settings=raster_settings
    ),
    shader=SoftPhongShader(
        device=device,
        cameras=cameras,
        lights=lights
    )
)
```

---



---

## 7. ä¿å­˜çš„ä½å§¿æ•°æ®ç»“æ„

ä½¿ç”¨ `--save_pose output.npy` å¯ä»¥ä¿å­˜æ‰‹éƒ¨ä½å§¿æ•°æ®ä¾›åç»­ä½¿ç”¨ã€‚

### 7.1 æ•°æ®ç»“æ„

```python
{
    'left': {
        frame_idx: {
            'wrist_position': np.ndarray,    # (3,) æ‰‹è…•3Dä½ç½® [x, y, z]
            'wrist_rotation': np.ndarray,    # (3, 3) æ‰‹è…•æ—‹è½¬çŸ©é˜µ
            'finger_rotations': np.ndarray,  # (15, 3, 3) æ‰‹æŒ‡å…³èŠ‚æ—‹è½¬çŸ©é˜µ
            'shape_params': np.ndarray,      # (10,) MANOå½¢çŠ¶å‚æ•°
        },
        ...  # å¤šå¸§æ•°æ®
    },
    'right': {
        frame_idx: {...},  # ä¸leftç»“æ„ç›¸åŒ
        ...
    },
    'description': {
        'wrist_position': 'æ‰‹è…•3Dä½ç½® (3,) - [x, y, z] åœ¨ç›¸æœºåæ ‡ç³»ä¸­',
        'wrist_rotation': 'æ‰‹è…•æ—‹è½¬çŸ©é˜µ (3, 3) - global_orient',
        'finger_rotations': '15ä¸ªæ‰‹æŒ‡å…³èŠ‚çš„æ—‹è½¬çŸ©é˜µ (15, 3, 3) - hand_pose',
        'shape_params': 'MANOå½¢çŠ¶å‚æ•° (10,) - beta',
        'note': 'ä½¿ç”¨è¿™äº›å‚æ•°å¯ä»¥é€šè¿‡MANOæ¨¡å‹é‡å»ºå®Œæ•´çš„æ‰‹éƒ¨ç½‘æ ¼å’Œå…³èŠ‚',
        'usage': 'é¡¶ç‚¹è®¡ç®—å…¬å¼: V_cam = global_orient @ (MANO(beta, hand_pose) - wrist) + transl'
    }
}
```

### 7.2 å‚æ•°è¯¦è§£

| å‚æ•° | å½¢çŠ¶ | æ•°æ®ç±»å‹ | è¯´æ˜ |
|------|------|---------|------|
| `wrist_position` | `(3,)` | `float32` | æ‰‹è…•åœ¨ç›¸æœºåæ ‡ç³»ä¸‹çš„3Dä½ç½® [x, y, z]ï¼ˆç±³ï¼‰ |
| `wrist_rotation` | `(3, 3)` | `float32` | æ‰‹è…•çš„å…¨å±€æ—‹è½¬çŸ©é˜µï¼ˆSO(3)ï¼‰ |
| `finger_rotations` | `(15, 3, 3)` | `float32` | 15ä¸ªæ‰‹æŒ‡å…³èŠ‚çš„å±€éƒ¨æ—‹è½¬çŸ©é˜µ |
| `shape_params` | `(10,)` | `float32` | MANO PCAå½¢çŠ¶å‚æ•°ï¼ˆæ§åˆ¶æ‰‹éƒ¨å¤§å°ã€ç²—ç»†ï¼‰ |

#### å…³èŠ‚ç´¢å¼•å¯¹åº”å…³ç³»

15ä¸ªæ‰‹æŒ‡å…³èŠ‚çš„ç´¢å¼•å¯¹åº”ï¼š

| å…³èŠ‚ç´¢å¼• | æ‰‹æŒ‡ | å…³èŠ‚åç§° | è¯´æ˜ |
|----------|------|---------|------|
| 0-2 | æ‹‡æŒ‡ | CMC, MCP, IP | 3ä¸ªå…³èŠ‚ |
| 3-5 | é£ŸæŒ‡ | MCP, PIP, DIP | 3ä¸ªå…³èŠ‚ |
| 6-8 | ä¸­æŒ‡ | MCP, PIP, DIP | 3ä¸ªå…³èŠ‚ |
| 9-11 | æ— åæŒ‡ | MCP, PIP, DIP | 3ä¸ªå…³èŠ‚ |
| 12-14 | å°æŒ‡ | MCP, PIP, DIP | 3ä¸ªå…³èŠ‚ |

**æ³¨æ„**ï¼š
- CMC: è…•æŒå…³èŠ‚
- MCP: æŒæŒ‡å…³èŠ‚  
- PIP: è¿‘ç«¯æŒ‡é—´å…³èŠ‚
- DIP: è¿œç«¯æŒ‡é—´å…³èŠ‚
- IP: æŒ‡é—´å…³èŠ‚ï¼ˆæ‹‡æŒ‡ï¼‰

### 7.3 åæ ‡ç³»ç»Ÿ

**ç›¸æœºåæ ‡ç³»**ï¼ˆå³æ‰‹ç³»ï¼‰ï¼š
- **Xè½´**ï¼šå‘å³ï¼ˆå›¾åƒå·¦â†’å³ï¼‰
- **Yè½´**ï¼šå‘ä¸‹ï¼ˆå›¾åƒä¸Šâ†’ä¸‹ï¼‰
- **Zè½´**ï¼šå‚ç›´äºå›¾åƒå¹³é¢å‘å‰ï¼ˆæ·±åº¦æ–¹å‘ï¼‰
- **åŸç‚¹**ï¼šç›¸æœºå…‰å¿ƒ

**å•ä½**ï¼šæ‰€æœ‰3Dåæ ‡çš„å•ä½ä¸º**ç±³**ã€‚

### 7.4 åŠ è½½å’Œä½¿ç”¨ç¤ºä¾‹

#### åŸºæœ¬åŠ è½½

```python
import numpy as np

# åŠ è½½æ•°æ®
data = np.load('hand_pose.npy', allow_pickle=True).item()

# æŸ¥çœ‹å¯ç”¨çš„å¸§
left_frames = list(data['left'].keys())
right_frames = list(data['right'].keys())
print(f"å·¦æ‰‹å¸§: {len(left_frames)}")
print(f"å³æ‰‹å¸§: {len(right_frames)}")

# è®¿é—®å·¦æ‰‹ç¬¬0å¸§æ•°æ®
if 0 in data['left']:
    left_frame_0 = data['left'][0]
    
    # æå–å‚æ•°
    wrist_pos = left_frame_0['wrist_position']      # (3,)
    wrist_rot = left_frame_0['wrist_rotation']      # (3, 3)
    finger_rot = left_frame_0['finger_rotations']   # (15, 3, 3)
    shape = left_frame_0['shape_params']            # (10,)
    
    print(f"æ‰‹è…•ä½ç½®: {wrist_pos}")
    print(f"å½¢çŠ¶å‚æ•°: {shape}")
```

#### é‡å»ºæ‰‹éƒ¨ç½‘æ ¼

```python
from libs.models.mano_wrapper import MANO
import torch

# åˆå§‹åŒ–MANOæ¨¡å‹
mano = MANO(model_path='./weights/mano')
device = 'cuda' if torch.cuda.is_available() else 'cpu'
mano = mano.to(device)

# åŠ è½½ä½å§¿æ•°æ®
data = np.load('hand_pose.npy', allow_pickle=True).item()
left_data = data['left'][0]

# è½¬æ¢ä¸ºTensor
shape = torch.tensor(left_data['shape_params']).unsqueeze(0).to(device)
finger_rot = torch.tensor(left_data['finger_rotations']).unsqueeze(0).to(device)
wrist_rot = torch.tensor(left_data['wrist_rotation']).to(device)
wrist_pos = torch.tensor(left_data['wrist_position']).to(device)

# MANOå‰å‘ä¼ æ’­ï¼ˆç”Ÿæˆå±€éƒ¨åæ ‡ç³»ä¸‹çš„ç½‘æ ¼ï¼‰
identity_rot = torch.eye(3).unsqueeze(0).unsqueeze(0).to(device)
output = mano(
    betas=shape,
    hand_pose=finger_rot,
    global_orient=identity_rot  # å…ˆä¸åº”ç”¨å…¨å±€æ—‹è½¬
)

vertices = output.vertices[0]  # (778, 3) æ‰‹éƒ¨é¡¶ç‚¹
joints = output.joints[0]      # (21, 3) æ‰‹éƒ¨å…³èŠ‚

# å·¦æ‰‹éœ€è¦Xè½´ç¿»è½¬
vertices[:, 0] = -vertices[:, 0]
joints[:, 0] = -joints[:, 0]

# åº”ç”¨å…¨å±€å˜æ¢åˆ°ç›¸æœºåæ ‡ç³»
wrist_joint = joints[0]  # æ‰‹è…•å…³èŠ‚ä½ç½®
vertices_cam = (wrist_rot @ (vertices - wrist_joint).T).T + wrist_pos
joints_cam = (wrist_rot @ (joints - wrist_joint).T).T + wrist_pos

print(f"é¡¶ç‚¹å½¢çŠ¶: {vertices_cam.shape}")  # (778, 3)
print(f"å…³èŠ‚å½¢çŠ¶: {joints_cam.shape}")    # (21, 3)
```

#### æŠ•å½±åˆ°2Då›¾åƒ

```python
import cv2

# å‡è®¾ç›¸æœºå†…å‚
fx, fy = 1000.0, 1000.0
cx, cy = 640.0, 360.0
K = np.array([
    [fx,  0, cx],
    [ 0, fy, cy],
    [ 0,  0,  1]
], dtype=np.float32)

# å°†3Då…³èŠ‚æŠ•å½±åˆ°2D
joints_2d = []
for joint_3d in joints_cam.cpu().numpy():
    # æŠ•å½±å…¬å¼: p_2d = K @ p_3d
    p_homo = K @ joint_3d
    u = p_homo[0] / p_homo[2]
    v = p_homo[1] / p_homo[2]
    joints_2d.append([u, v])

joints_2d = np.array(joints_2d)

# åœ¨å›¾åƒä¸Šç»˜åˆ¶å…³é”®ç‚¹
image = cv2.imread('frame_0.jpg')
for u, v in joints_2d:
    cv2.circle(image, (int(u), int(v)), 3, (0, 255, 0), -1)
cv2.imshow('Joints 2D', image)
cv2.waitKey(0)
```

#### æ‰¹é‡å¤„ç†å¤šå¸§

```python
# æ‰¹é‡æå–æ‰€æœ‰å·¦æ‰‹æ•°æ®
all_left_positions = []
all_left_rotations = []

for frame_idx in sorted(data['left'].keys()):
    frame_data = data['left'][frame_idx]
    all_left_positions.append(frame_data['wrist_position'])
    all_left_rotations.append(frame_data['wrist_rotation'])

# è½¬æ¢ä¸ºæ•°ç»„
positions = np.array(all_left_positions)  # (T, 3)
rotations = np.array(all_left_rotations)  # (T, 3, 3)

print(f"å·¦æ‰‹è½¨è¿¹é•¿åº¦: {len(positions)} å¸§")
print(f"å¹³å‡æ‰‹è…•ä½ç½®: {positions.mean(axis=0)}")
```

### 7.5 æ•°æ®éªŒè¯

#### æ£€æŸ¥æ—‹è½¬çŸ©é˜µæœ‰æ•ˆæ€§

```python
def is_valid_rotation_matrix(R, eps=1e-5):
    """éªŒè¯æ—‹è½¬çŸ©é˜µçš„æœ‰æ•ˆæ€§"""
    # æ£€æŸ¥ R @ R.T = I
    should_be_identity = R @ R.T
    identity = np.eye(3)
    if not np.allclose(should_be_identity, identity, atol=eps):
        return False
    
    # æ£€æŸ¥ det(R) = 1
    det = np.linalg.det(R)
    if not np.isclose(det, 1.0, atol=eps):
        return False
    
    return True

# éªŒè¯æ•°æ®
data = np.load('hand_pose.npy', allow_pickle=True).item()
for frame_idx, frame_data in data['left'].items():
    wrist_rot = frame_data['wrist_rotation']
    if not is_valid_rotation_matrix(wrist_rot):
        print(f"è­¦å‘Š: å¸§ {frame_idx} çš„æ—‹è½¬çŸ©é˜µæ— æ•ˆï¼")
    
    # æ£€æŸ¥æ¯ä¸ªæ‰‹æŒ‡å…³èŠ‚æ—‹è½¬
    for i, joint_rot in enumerate(frame_data['finger_rotations']):
        if not is_valid_rotation_matrix(joint_rot):
            print(f"è­¦å‘Š: å¸§ {frame_idx} çš„å…³èŠ‚ {i} æ—‹è½¬çŸ©é˜µæ— æ•ˆï¼")
```

#### æ£€æŸ¥æ•°æ®å®Œæ•´æ€§

```python
def check_data_integrity(npy_file):
    """æ£€æŸ¥ä¿å­˜çš„npyæ–‡ä»¶çš„å®Œæ•´æ€§"""
    data = np.load(npy_file, allow_pickle=True).item()
    
    # æ£€æŸ¥å¿…éœ€çš„é”®
    assert 'left' in data, "ç¼ºå°‘ 'left' é”®"
    assert 'right' in data, "ç¼ºå°‘ 'right' é”®"
    assert 'description' in data, "ç¼ºå°‘ 'description' é”®"
    
    # æ£€æŸ¥æ¯å¸§æ•°æ®
    for hand_type in ['left', 'right']:
        for frame_idx, frame_data in data[hand_type].items():
            # æ£€æŸ¥å¿…éœ€çš„å­—æ®µ
            required_keys = ['wrist_position', 'wrist_rotation', 
                           'finger_rotations', 'shape_params']
            for key in required_keys:
                assert key in frame_data, f"å¸§ {frame_idx} ç¼ºå°‘ '{key}'"
            
            # æ£€æŸ¥å½¢çŠ¶
            assert frame_data['wrist_position'].shape == (3,)
            assert frame_data['wrist_rotation'].shape == (3, 3)
            assert frame_data['finger_rotations'].shape == (15, 3, 3)
            assert frame_data['shape_params'].shape == (10,)
    
    print("âœ… æ•°æ®å®Œæ•´æ€§æ£€æŸ¥é€šè¿‡ï¼")
    return True

check_data_integrity('hand_pose.npy')
```

### 7.6 åº”ç”¨åœºæ™¯

ä¿å­˜çš„ä½å§¿æ•°æ®å¯ç”¨äºï¼š

1. **åŠ¨ç”»åˆ¶ä½œ**ï¼šå°†æ‰‹éƒ¨åŠ¨ä½œå¯¼å…¥3Dè½¯ä»¶
2. **æ‰‹åŠ¿è¯†åˆ«**ï¼šæå–æ‰‹éƒ¨ç‰¹å¾è¿›è¡Œåˆ†ç±»
3. **è™šæ‹Ÿç°å®**ï¼šå®æ—¶æ‰‹éƒ¨è¿½è¸ªä¸äº¤äº’
4. **æ•°æ®å¢å¼º**ï¼šç”Ÿæˆæ›´å¤šè®­ç»ƒæ•°æ®
5. **è¿åŠ¨åˆ†æ**ï¼šåˆ†ææ‰‹éƒ¨è¿åŠ¨è½¨è¿¹
6. **æœºå™¨äººæ§åˆ¶**ï¼šå°†äººæ‰‹åŠ¨ä½œæ˜ å°„åˆ°æœºå™¨äººæ‰‹

---

## 8. å¸¸è§é—®é¢˜ä¸è°ƒè¯•

### 9.1 æ‰‹éƒ¨ä½ç½®ä¸å‡†ç¡®

**ç—‡çŠ¶**ï¼šæ¸²æŸ“çš„æ‰‹éƒ¨ä¸å®é™…ä½ç½®åç§»

**å¯èƒ½åŸå› **ï¼š
1. ç›¸æœºå†…å‚ä¸å‡†ç¡®
2. ç›¸æœºä¸»ç‚¹ (cx, cy) è®¾ç½®é”™è¯¯

**è§£å†³æ–¹æ³•**ï¼š
```python
# æ£€æŸ¥ä¸»ç‚¹æ˜¯å¦ä¸ºå›¾åƒä¸­å¿ƒ
cx_expected = W / 2
cy_expected = H / 2

# å¦‚æœä¸æ˜¯ï¼Œæ‰‹åŠ¨æŒ‡å®š
--camera_cx {cx_expected} --camera_cy {cy_expected}
```

### 9.2 æ‰‹éƒ¨æœå‘é”™è¯¯

**ç—‡çŠ¶**ï¼šæ‰‹æŒæœå‘ä¸å®é™…ä¸ç¬¦

**å¯èƒ½åŸå› **ï¼š
1. å·¦å³æ‰‹è¯†åˆ«é”™è¯¯
2. å…¨å±€æ—‹è½¬ä¼°è®¡ä¸å‡†

**è§£å†³æ–¹æ³•**ï¼š
```python
# æ£€æŸ¥æ£€æµ‹å™¨è¾“å‡º
print(f"Hand type: {hand_type}")  # åº”ä¸º 'left' æˆ– 'right'

# æ£€æŸ¥å…¨å±€æ—‹è½¬
print(f"Global orient:\n{global_orient}")
```

### 9.3 æ¸²æŸ“å¤±è´¥

**ç—‡çŠ¶**ï¼šPyTorch3DæŠ¥é”™æˆ–è¾“å‡ºé»‘å±

**å¯èƒ½åŸå› **ï¼š
1. CUDAç‰ˆæœ¬ä¸å…¼å®¹
2. PyTorch3Dæœªæ­£ç¡®å®‰è£…

**è§£å†³æ–¹æ³•**ï¼š
```bash
# æ£€æŸ¥PyTorch3D
python -c "import pytorch3d; print(pytorch3d.__version__)"

# é‡æ–°å®‰è£…ï¼ˆä¸PyTorchç‰ˆæœ¬åŒ¹é…ï¼‰
pip install pytorch3d -f https://dl.fbaipublicfiles.com/pytorch3d/packaging/wheels/...
```

### 9.4 æ€§èƒ½ä¼˜åŒ–

**é—®é¢˜**ï¼šå¤„ç†é€Ÿåº¦æ…¢

**ä¼˜åŒ–å»ºè®®**ï¼š

1. **ä½¿ç”¨GPU**
   ```bash
   --device cuda
   ```

2. **å‡å°‘å¤„ç†å¸§æ•°**ï¼ˆæµ‹è¯•æ—¶ï¼‰
   ```bash
   --max_frames 100
   ```

3. **é™ä½æ£€æµ‹é˜ˆå€¼**ï¼ˆå‡å°‘è¯¯æ£€é‡è¯•ï¼‰
   ```bash
   --thresh 0.6
   ```

4. **ä½¿ç”¨æ‰¹å¤„ç†**ï¼ˆä¿®æ”¹ä»£ç ï¼‰
   ```python
   # æ‰¹é‡æ¨ç†ï¼ˆéœ€è¦ä¿®æ”¹HaWoRè°ƒç”¨ï¼‰
   batch_size = 4
   ```

---

## 9. æ‰©å±•ä¸æ”¹è¿›æ–¹å‘

### 10.1 å¯èƒ½çš„æ”¹è¿›

1. **æ—¶åºå¹³æ»‘**
   - æ·»åŠ å¡å°”æ›¼æ»¤æ³¢æˆ–ç§»åŠ¨å¹³å‡
   - å‡å°‘å¸§é—´æŠ–åŠ¨

2. **å¤šè§†è§’èåˆ**
   - æ”¯æŒå¤šä¸ªç›¸æœº
   - æé«˜é‡å»ºé²æ£’æ€§

3. **äº¤äº’å¼è°ƒæ•´**
   - å®æ—¶é¢„è§ˆ
   - æ‰‹åŠ¨è°ƒæ•´å‚æ•°

4. **åå¤„ç†ä¼˜åŒ–**
   - ç¢°æ’æ£€æµ‹
   - æ‰‹éƒ¨è‡ªé®æŒ¡å¤„ç†

### 10.2 åº”ç”¨åœºæ™¯

- **æ‰‹è¯­è¯†åˆ«**ï¼šæå–æ‰‹åŠ¿ç‰¹å¾
- **è™šæ‹Ÿç°å®**ï¼šæ‰‹éƒ¨è¿½è¸ªä¸äº¤äº’
- **åŠ¨ä½œæ•æ‰**ï¼šç”ŸæˆåŠ¨ç”»æ•°æ®
- **åŒ»ç–—åˆ†æ**ï¼šæ‰‹éƒ¨åŠŸèƒ½è¯„ä¼°

---

## å‚è€ƒèµ„æ–™

- **HaWoR**: [Hand-and-Wrist-based 3D Hand Pose Estimation](https://github.com/LinHuang17/HaWoR)
- **MANO**: [MANO: Modeling and Capturing Hands and Bodies Together](https://mano.is.tue.mpg.de/)
- **PyTorch3D**: [PyTorch3D Documentation](https://pytorch3d.org/)
- **ç›¸æœºæ ‡å®š**: [OpenCV Camera Calibration](https://docs.opencv.org/master/dc/dbb/tutorial_py_calibration.html)

---

## æ€»ç»“

æœ¬é¡¹ç›®å®ç°äº†ä¸€ä¸ª**é«˜æ•ˆã€å‡†ç¡®ã€æ˜“ç”¨**çš„3Dæ‰‹éƒ¨é‡å»ºæµæ°´çº¿ï¼Œé€šè¿‡ä½¿ç”¨å·²çŸ¥ç›¸æœºå†…å‚ï¼Œé¿å…äº†ç›¸æœºå‚æ•°ä¼°è®¡çš„å¤æ‚æ€§å’Œä¸ç¡®å®šæ€§ï¼Œæä¾›äº†æ›´å¿«çš„å¤„ç†é€Ÿåº¦å’Œæ›´é«˜çš„é‡å»ºè´¨é‡ã€‚

æ ¸å¿ƒåˆ›æ–°ç‚¹ï¼š
1. **ç®€åŒ–æµç¨‹**ï¼šç§»é™¤MoGeä¼°è®¡ç¯èŠ‚
2. **åæ ‡å¯¹é½**ï¼šä¿®æ­£MANOè¾“å‡ºåˆ°ç›¸æœºåæ ‡ç³»
3. **é«˜è´¨é‡æ¸²æŸ“**ï¼šé›†æˆPyTorch3Dä¸“ä¸šæ¸²æŸ“
4. **æ•°æ®å¯¼å‡º**ï¼šæ”¯æŒä½å§¿æ•°æ®ä¿å­˜ä¸å¤ç”¨

è¯¥ç³»ç»Ÿé€‚ç”¨äºå·²æ ‡å®šç›¸æœºçš„åœºæ™¯ï¼Œå¦‚æœºå™¨äººè§†è§‰ã€AR/VRåº”ç”¨ã€æ‰‹åŠ¿è¯†åˆ«ç­‰é¢†åŸŸã€‚
